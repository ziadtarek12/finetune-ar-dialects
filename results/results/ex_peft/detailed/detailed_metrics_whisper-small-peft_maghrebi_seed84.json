{
  "experiment_metadata": {
    "experiment_name": "whisper-small-peft_maghrebi_seed84",
    "model_name": "whisper-small",
    "dialect": "maghrebi",
    "method": "peft",
    "seed": 84,
    "start_time": "2025-09-26 04:54:48",
    "end_time": "2025-09-26 08:26:58",
    "training_time_seconds": 12729.868757009506,
    "peak_memory_mb": 1292.97705078125,
    "total_params": 245273856,
    "trainable_params": 3538944,
    "trainable_percentage": 1.442854145857274,
    "wer": 89.79958890030832,
    "cer": 58.48358912268071,
    "final_loss": 1.5530004577636718,
    "lora_config": {},
    "parameter_efficiency_ratio": 0,
    "memory_efficiency_ratio": 0.007889089055005521,
    "training_efficiency_score": 2.8846707424748526,
    "convergence_step": 0,
    "gradient_norm_stats": {},
    "layer_adaptation_stats": {},
    "performance_per_param": 2.8823318763144257,
    "lora_rank": 0,
    "lora_alpha": 0,
    "lora_dropout": 0,
    "target_modules_count": 0,
    "adapter_weights_norm": 0,
    "base_model_frozen_params": 0,
    "effective_rank": 0,
    "adaptation_magnitude": 0,
    "model_type": "PEFT_LoRA"
  },
  "step_by_step_metrics": [],
  "layer_adaptation_history": {},
  "gradient_evolution": [],
  "training_summary": {
    "experiment_type": "PEFT training on maghrebi dialect",
    "model_efficiency": {
      "trainable_params_percentage": 1.442854145857274,
      "memory_efficiency": 0.007889089055005521,
      "parameter_efficiency": 2.8823318763144257
    },
    "training_dynamics": {
      "total_steps": 0,
      "convergence_analysis": {
        "status": "insufficient_data"
      },
      "gradient_stability": {
        "status": "insufficient_data"
      }
    },
    "performance_metrics": {
      "final_wer": 89.79958890030832,
      "final_cer": 58.48358912268071,
      "loss_reduction": 0
    },
    "lora_analysis": {
      "effective_rank": 0,
      "adaptation_strength": 0,
      "layer_utilization": {}
    }
  }
}