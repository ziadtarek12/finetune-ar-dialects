{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89a190b1",
   "metadata": {},
   "source": [
    "# Enhanced Arabic Dialect PEFT Training Metrics Analysis\n",
    "\n",
    "This notebook provides comprehensive analysis of training metrics recorded during Arabic dialect fine-tuning experiments.\n",
    "It includes advanced LORA effectiveness analysis, real-time monitoring capabilities, and detailed visualization of PEFT impact.\n",
    "\n",
    "**Enhanced Features:**\n",
    "- LORA adapter effectiveness quantification\n",
    "- Layer-wise adaptation analysis\n",
    "- Parameter efficiency tracking  \n",
    "- Cross-dialect generalization assessment\n",
    "- Real-time training monitoring\n",
    "- Interactive visualization dashboard\n",
    "\n",
    "**Data Source:** Enhanced metrics automatically recorded by `dialect_peft_training.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e30e1f",
   "metadata": {},
   "source": [
    "## 1. Import Enhanced Libraries\n",
    "Import comprehensive libraries including advanced visualization, statistical analysis, and interactive plotting capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7965f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import datetime\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import networkx as nx\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style for enhanced aesthetics\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure matplotlib with enhanced settings\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (14, 10),\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 16,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'legend.fontsize': 12,\n",
    "    'figure.dpi': 100,\n",
    "    'savefig.dpi': 300,\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3\n",
    "})\n",
    "\n",
    "# Configure plotly for notebook\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "\n",
    "print(\"Enhanced libraries imported successfully!\")\n",
    "print(f\"Analysis timestamp: {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1dc647",
   "metadata": {},
   "source": [
    "## 2. Enhanced Data Loading and Processing\n",
    "Load comprehensive training data with support for detailed LORA metrics and step-by-step analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b098fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_enhanced_training_metrics(base_dir=\"./results\"):\n",
    "    \"\"\"Load comprehensive training metrics including detailed LORA analysis.\"\"\"\n",
    "    \n",
    "    # Look in enhanced directory structure\n",
    "    subdirs = [\"ex_finetune\", \"ex_scratch\", \"ex_peft\", \"ex_comparison_dialectal\"]\n",
    "    all_metrics = []\n",
    "    detailed_metrics = []\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        subdir_path = Path(base_dir) / subdir\n",
    "        if subdir_path.exists():\n",
    "            # Load standard metrics files\n",
    "            metrics_files = list(subdir_path.glob(\"results_whisper-*.json\"))\n",
    "            \n",
    "            # Load detailed metrics if available\n",
    "            detailed_dir = subdir_path / \"detailed\"\n",
    "            if detailed_dir.exists():\n",
    "                detailed_files = list(detailed_dir.glob(\"detailed_metrics_*.json\"))\n",
    "            else:\n",
    "                detailed_files = []\n",
    "            \n",
    "            for file_path in metrics_files:\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                    \n",
    "                    # Parse filename for metadata\n",
    "                    filename = file_path.stem\n",
    "                    parts = filename.split('_')\n",
    "                    \n",
    "                    if len(parts) >= 3:\n",
    "                        model_part = parts[1]\n",
    "                        dialect = parts[2] if len(parts) > 2 else \"unknown\"\n",
    "                        seed = parts[3].replace(\"seed\", \"\") if len(parts) > 3 and \"seed\" in parts[3] else \"42\"\n",
    "                        \n",
    "                        # Determine method\n",
    "                        if \"peft\" in model_part:\n",
    "                            method = \"PEFT_LoRA\"\n",
    "                        elif \"finetune\" in model_part:\n",
    "                            method = \"Full_FineTune\"\n",
    "                        else:\n",
    "                            method = \"Unknown\"\n",
    "                        \n",
    "                        # Enhanced metrics structure\n",
    "                        metrics = {\n",
    "                            'experiment_name': filename,\n",
    "                            'dialect': dialect,\n",
    "                            'model_type': method,\n",
    "                            'seed': int(seed),\n",
    "                            'final_wer': data.get('wer', data.get('final_wer', 0)),\n",
    "                            'final_cer': data.get('cer', data.get('final_cer', 0)),\n",
    "                            'training_time_seconds': data.get('training_time_seconds', data.get('training_time', 0)),\n",
    "                            'peak_memory_mb': data.get('peak_memory_mb', 0),\n",
    "                            'trainable_params': data.get('trainable_params', 0),\n",
    "                            'total_params': data.get('total_params', 0),\n",
    "                            'final_loss': data.get('final_loss', 0),\n",
    "                            \n",
    "                            # Enhanced LORA metrics (if available)\n",
    "                            'lora_rank': data.get('lora_rank', 0),\n",
    "                            'lora_alpha': data.get('lora_alpha', 0),\n",
    "                            'lora_dropout': data.get('lora_dropout', 0),\n",
    "                            'parameter_efficiency_ratio': data.get('parameter_efficiency_ratio', 0),\n",
    "                            'memory_efficiency_ratio': data.get('memory_efficiency_ratio', 0),\n",
    "                            'training_efficiency_score': data.get('training_efficiency_score', 0),\n",
    "                            'convergence_step': data.get('convergence_step', 0),\n",
    "                            'effective_rank': data.get('effective_rank', 0),\n",
    "                            'adaptation_magnitude': data.get('adaptation_magnitude', 0),\n",
    "                            'performance_per_param': data.get('performance_per_param', 0),\n",
    "                            \n",
    "                            'source_file': str(file_path)\n",
    "                        }\n",
    "                        \n",
    "                        all_metrics.append(metrics)\n",
    "                        print(f\"Loaded: {file_path} -> {method} {dialect}\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_path}: {e}\")\n",
    "            \n",
    "            # Load detailed metrics\n",
    "            for detailed_file in detailed_files:\n",
    "                try:\n",
    "                    with open(detailed_file, 'r') as f:\n",
    "                        detailed_data = json.load(f)\n",
    "                    detailed_metrics.append({\n",
    "                        'filename': detailed_file.stem,\n",
    "                        'data': detailed_data\n",
    "                    })\n",
    "                    print(f\"Loaded detailed: {detailed_file}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading detailed {detailed_file}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrames\n",
    "    metrics_df = pd.DataFrame(all_metrics) if all_metrics else pd.DataFrame()\n",
    "    \n",
    "    if not metrics_df.empty:\n",
    "        print(f\"\\nLoaded {len(metrics_df)} experiments\")\n",
    "        print(f\"Methods found: {metrics_df['model_type'].unique()}\")\n",
    "        print(f\"Dialects found: {metrics_df['dialect'].unique()}\")\n",
    "        \n",
    "        # Enhanced data validation\n",
    "        print(f\"\\nData completeness:\")\n",
    "        print(f\"- Basic metrics: {metrics_df[['final_wer', 'final_cer', 'training_time_seconds']].notna().all(axis=1).sum()}/{len(metrics_df)}\")\n",
    "        print(f\"- LORA metrics: {metrics_df[['lora_rank', 'parameter_efficiency_ratio']].notna().all(axis=1).sum()}/{len(metrics_df)}\")\n",
    "        print(f\"- Efficiency metrics: {metrics_df[['memory_efficiency_ratio', 'training_efficiency_score']].notna().all(axis=1).sum()}/{len(metrics_df)}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No metrics files found.\")\n",
    "        print(\"Make sure you've run enhanced training experiments that save results in:\")\n",
    "        print(\"  - ./results/ex_peft/\")\n",
    "        print(\"  - ./results/ex_finetune/\")\n",
    "        \n",
    "    return metrics_df, detailed_metrics\n",
    "\n",
    "# Load the enhanced data\n",
    "metrics_df, detailed_metrics = load_enhanced_training_metrics()\n",
    "\n",
    "if not metrics_df.empty:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ENHANCED DATASET OVERVIEW\")\n",
    "    print(\"=\"*60)\n",
    "    print(metrics_df.head())\n",
    "    print(f\"\\nColumns: {list(metrics_df.columns)}\")\n",
    "    print(f\"\\nDataset shape: {metrics_df.shape}\")\n",
    "else:\n",
    "    print(\"No data loaded. Please run enhanced training experiments first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644445e2",
   "metadata": {},
   "source": [
    "## 3. Enhanced Data Preprocessing and Feature Engineering\n",
    "Advanced preprocessing including efficiency metrics calculation, normalization, and feature engineering for LORA analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816993f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_preprocess_metrics(df):\n",
    "    \"\"\"Enhanced preprocessing with comprehensive feature engineering.\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    # Convert time units\n",
    "    df['training_time_minutes'] = df['training_time_seconds'] / 60\n",
    "    df['training_time_hours'] = df['training_time_seconds'] / 3600\n",
    "    \n",
    "    # Convert memory units\n",
    "    df['peak_memory_gb'] = df['peak_memory_mb'] / 1024\n",
    "    \n",
    "    # Convert parameters to millions\n",
    "    df['total_params_millions'] = df['total_params'] / 1_000_000\n",
    "    df['trainable_params_millions'] = df['trainable_params'] / 1_000_000\n",
    "    \n",
    "    # Calculate trainable percentage\n",
    "    df['trainable_percentage'] = (df['trainable_params'] / df['total_params']) * 100\n",
    "    \n",
    "    # Enhanced efficiency metrics\n",
    "    df['wer_score'] = 100 - df['final_wer']  # Higher is better\n",
    "    df['cer_score'] = 100 - df['final_cer']  # Higher is better\n",
    "    \n",
    "    # Memory efficiency (performance per GB)\n",
    "    df['memory_efficiency'] = df['wer_score'] / (df['peak_memory_gb'] + 0.1)  # Add small constant to avoid division by zero\n",
    "    \n",
    "    # Parameter efficiency (performance per million trainable parameters)\n",
    "    df['param_efficiency'] = df['wer_score'] / (df['trainable_params_millions'] + 0.1)\n",
    "    \n",
    "    # Time efficiency (performance per hour)\n",
    "    df['time_efficiency'] = df['wer_score'] / (df['training_time_hours'] + 0.01)\n",
    "    \n",
    "    # LORA-specific metrics\n",
    "    peft_mask = df['model_type'] == 'PEFT_LoRA'\n",
    "    \n",
    "    if peft_mask.any():\n",
    "        # LORA effectiveness score\n",
    "        df.loc[peft_mask, 'lora_effectiveness'] = (\n",
    "            df.loc[peft_mask, 'wer_score'] * \n",
    "            df.loc[peft_mask, 'parameter_efficiency_ratio'] * 100\n",
    "        )\n",
    "        \n",
    "        # Rank efficiency\n",
    "        df.loc[peft_mask, 'rank_efficiency'] = (\n",
    "            df.loc[peft_mask, 'effective_rank'] / (df.loc[peft_mask, 'lora_rank'] + 1)\n",
    "        )\n",
    "        \n",
    "        # Adaptation efficiency\n",
    "        df.loc[peft_mask, 'adaptation_efficiency'] = (\n",
    "            df.loc[peft_mask, 'wer_score'] / (df.loc[peft_mask, 'adaptation_magnitude'] + 1e-6)\n",
    "        )\n",
    "    \n",
    "    # Composite efficiency score\n",
    "    df['composite_efficiency'] = (\n",
    "        0.4 * df['param_efficiency'] + \n",
    "        0.3 * df['memory_efficiency'] + \n",
    "        0.3 * df['time_efficiency']\n",
    "    )\n",
    "    \n",
    "    # Performance tier classification\n",
    "    df['performance_tier'] = pd.cut(\n",
    "        df['final_wer'], \n",
    "        bins=[0, 30, 50, 70, 100], \n",
    "        labels=['Excellent', 'Good', 'Fair', 'Poor']\n",
    "    )\n",
    "    \n",
    "    # Resource usage tier\n",
    "    df['resource_tier'] = pd.cut(\n",
    "        df['peak_memory_gb'], \n",
    "        bins=[0, 2, 5, 10, float('inf')], \n",
    "        labels=['Low', 'Medium', 'High', 'Very High']\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Enhanced preprocessing\n",
    "if not metrics_df.empty:\n",
    "    metrics_df = enhanced_preprocess_metrics(metrics_df)\n",
    "    \n",
    "    print(\"Enhanced preprocessing complete!\")\n",
    "    print(f\"\\nEnhanced dataset overview:\")\n",
    "    \n",
    "    # Display key efficiency metrics\n",
    "    efficiency_cols = ['param_efficiency', 'memory_efficiency', 'time_efficiency', \n",
    "                      'composite_efficiency', 'performance_tier', 'resource_tier']\n",
    "    \n",
    "    available_efficiency_cols = [col for col in efficiency_cols if col in metrics_df.columns]\n",
    "    if available_efficiency_cols:\n",
    "        print(\"\\nEfficiency Metrics Summary:\")\n",
    "        print(metrics_df[available_efficiency_cols].describe())\n",
    "    \n",
    "    # LORA-specific analysis\n",
    "    peft_data = metrics_df[metrics_df['model_type'] == 'PEFT_LoRA']\n",
    "    if not peft_data.empty:\n",
    "        print(f\"\\nLORA Experiments: {len(peft_data)}\")\n",
    "        lora_cols = ['lora_rank', 'lora_alpha', 'parameter_efficiency_ratio', \n",
    "                    'lora_effectiveness', 'rank_efficiency']\n",
    "        available_lora_cols = [col for col in lora_cols if col in peft_data.columns]\n",
    "        if available_lora_cols:\n",
    "            print(\"LORA Metrics Summary:\")\n",
    "            print(peft_data[available_lora_cols].describe())\n",
    "    \n",
    "    print(f\"\\nFinal dataset shape: {metrics_df.shape}\")\n",
    "    print(f\"Methods: {metrics_df['model_type'].value_counts().to_dict()}\")\n",
    "    print(f\"Dialects: {metrics_df['dialect'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f77c57",
   "metadata": {},
   "source": [
    "## 4. Enhanced Performance Analysis\n",
    "Comprehensive performance analysis including LORA effectiveness, efficiency metrics, and cross-dialect comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99110d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_performance_analysis(df):\n",
    "    \"\"\"Create comprehensive performance analysis with enhanced visualizations.\"\"\"\n",
    "    if df.empty or 'final_wer' not in df.columns:\n",
    "        print(\"No WER data available for analysis.\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with multiple subplots\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # 1. Performance comparison by method and dialect\n",
    "    ax1 = plt.subplot(3, 3, 1)\n",
    "    if 'model_type' in df.columns and 'dialect' in df.columns:\n",
    "        pivot_wer = df.pivot_table(values='final_wer', index='dialect', columns='model_type', aggfunc='mean')\n",
    "        sns.heatmap(pivot_wer, annot=True, fmt='.1f', cmap='RdYlGn_r', ax=ax1)\n",
    "        ax1.set_title('Average WER by Method and Dialect', fontweight='bold')\n",
    "    \n",
    "    # 2. Efficiency scatter plot\n",
    "    ax2 = plt.subplot(3, 3, 2)\n",
    "    if 'param_efficiency' in df.columns and 'memory_efficiency' in df.columns:\n",
    "        scatter = ax2.scatter(df['param_efficiency'], df['memory_efficiency'], \n",
    "                            c=df['final_wer'], s=80, alpha=0.7, cmap='RdYlGn_r')\n",
    "        ax2.set_xlabel('Parameter Efficiency')\n",
    "        ax2.set_ylabel('Memory Efficiency')\n",
    "        ax2.set_title('Efficiency Analysis', fontweight='bold')\n",
    "        plt.colorbar(scatter, ax=ax2, label='WER')\n",
    "    \n",
    "    # 3. Training time vs performance\n",
    "    ax3 = plt.subplot(3, 3, 3)\n",
    "    if 'training_time_hours' in df.columns:\n",
    "        for method in df['model_type'].unique():\n",
    "            method_data = df[df['model_type'] == method]\n",
    "            ax3.scatter(method_data['training_time_hours'], method_data['final_wer'], \n",
    "                       label=method, alpha=0.7, s=60)\n",
    "        ax3.set_xlabel('Training Time (hours)')\n",
    "        ax3.set_ylabel('WER (%)')\n",
    "        ax3.set_title('Training Time vs Performance', fontweight='bold')\n",
    "        ax3.legend()\n",
    "    \n",
    "    # 4. LORA-specific analysis\n",
    "    ax4 = plt.subplot(3, 3, 4)\n",
    "    peft_data = df[df['model_type'] == 'PEFT_LoRA']\n",
    "    if not peft_data.empty and 'lora_rank' in peft_data.columns:\n",
    "        rank_performance = peft_data.groupby('lora_rank')['final_wer'].agg(['mean', 'std']).reset_index()\n",
    "        ax4.errorbar(rank_performance['lora_rank'], rank_performance['mean'], \n",
    "                    yerr=rank_performance['std'], marker='o', capsize=5)\n",
    "        ax4.set_xlabel('LoRA Rank')\n",
    "        ax4.set_ylabel('Average WER (%)')\n",
    "        ax4.set_title('LoRA Rank vs Performance', fontweight='bold')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Parameter efficiency comparison\n",
    "    ax5 = plt.subplot(3, 3, 5)\n",
    "    if 'trainable_percentage' in df.columns:\n",
    "        df.boxplot(column='trainable_percentage', by='model_type', ax=ax5)\n",
    "        ax5.set_xlabel('Method')\n",
    "        ax5.set_ylabel('Trainable Parameters (%)')\n",
    "        ax5.set_title('Parameter Efficiency by Method', fontweight='bold')\n",
    "        plt.suptitle('')  # Remove automatic title\n",
    "    \n",
    "    # 6. Memory usage analysis\n",
    "    ax6 = plt.subplot(3, 3, 6)\n",
    "    if 'peak_memory_gb' in df.columns:\n",
    "        df.boxplot(column='peak_memory_gb', by='model_type', ax=ax6)\n",
    "        ax6.set_xlabel('Method')\n",
    "        ax6.set_ylabel('Peak Memory (GB)')\n",
    "        ax6.set_title('Memory Usage by Method', fontweight='bold')\n",
    "        plt.suptitle('')\n",
    "    \n",
    "    # 7. Performance distribution\n",
    "    ax7 = plt.subplot(3, 3, 7)\n",
    "    for method in df['model_type'].unique():\n",
    "        method_data = df[df['model_type'] == method]['final_wer']\n",
    "        ax7.hist(method_data, alpha=0.6, label=method, bins=15)\n",
    "    ax7.set_xlabel('WER (%)')\n",
    "    ax7.set_ylabel('Frequency')\n",
    "    ax7.set_title('Performance Distribution', fontweight='bold')\n",
    "    ax7.legend()\n",
    "    \n",
    "    # 8. Composite efficiency analysis\n",
    "    ax8 = plt.subplot(3, 3, 8)\n",
    "    if 'composite_efficiency' in df.columns:\n",
    "        df.boxplot(column='composite_efficiency', by='dialect', ax=ax8)\n",
    "        ax8.set_xlabel('Dialect')\n",
    "        ax8.set_ylabel('Composite Efficiency Score')\n",
    "        ax8.set_title('Efficiency by Dialect', fontweight='bold')\n",
    "        ax8.tick_params(axis='x', rotation=45)\n",
    "        plt.suptitle('')\n",
    "    \n",
    "    # 9. Performance tier analysis\n",
    "    ax9 = plt.subplot(3, 3, 9)\n",
    "    if 'performance_tier' in df.columns:\n",
    "        tier_counts = df.groupby(['model_type', 'performance_tier']).size().unstack(fill_value=0)\n",
    "        tier_counts.plot(kind='bar', stacked=True, ax=ax9)\n",
    "        ax9.set_xlabel('Method')\n",
    "        ax9.set_ylabel('Count')\n",
    "        ax9.set_title('Performance Tier Distribution', fontweight='bold')\n",
    "        ax9.tick_params(axis='x', rotation=45)\n",
    "        ax9.legend(title='Performance Tier')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\n",
    "\" + \"=\"*80)\n",
    "    print(\"ENHANCED PERFORMANCE ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"\n",
    "Overall Statistics:\")\n",
    "    print(f\"Total experiments: {len(df)}\")\n",
    "    print(f\"Methods: {df['model_type'].value_counts().to_dict()}\")\n",
    "    print(f\"Dialects: {df['dialect'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Performance by method\n",
    "    print(f\"\n",
    "Performance by Method (WER %):\")\n",
    "    method_performance = df.groupby('model_type')['final_wer'].agg(['mean', 'std', 'min', 'max'])\n",
    "    print(method_performance.round(2))\n",
    "    \n",
    "    # LORA effectiveness\n",
    "    peft_data = df[df['model_type'] == 'PEFT_LoRA']\n",
    "    if not peft_data.empty:\n",
    "        print(f\"\n",
    "LORA Effectiveness Analysis:\")\n",
    "        if 'parameter_efficiency_ratio' in peft_data.columns:\n",
    "            avg_param_efficiency = peft_data['parameter_efficiency_ratio'].mean()\n",
    "            print(f\"Average parameter efficiency: {avg_param_efficiency:.4f} ({avg_param_efficiency*100:.2f}% of base model)\")\n",
    "        \n",
    "        if 'lora_effectiveness' in peft_data.columns:\n",
    "            print(f\"Average LORA effectiveness score: {peft_data['lora_effectiveness'].mean():.2f}\")\n",
    "        \n",
    "        if 'rank_efficiency' in peft_data.columns:\n",
    "            print(f\"Average rank efficiency: {peft_data['rank_efficiency'].mean():.2f}\")\n",
    "\n",
    "# Run enhanced performance analysis\n",
    "if not metrics_df.empty:\n",
    "    create_enhanced_performance_analysis(metrics_df)\n",
    "else:\n",
    "    print(\"No data available for performance analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3a0f75",
   "metadata": {},
   "source": [
    "## 5. Interactive LORA Analysis Dashboard\n",
    "Interactive visualizations using Plotly for deep LORA effectiveness analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5454009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_lora_dashboard(df):\n",
    "    \"\"\"Create interactive dashboard for LORA analysis.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"No data available for interactive analysis.\")\n",
    "        return\n",
    "    \n",
    "    peft_data = df[df['model_type'] == 'PEFT_LoRA'].copy()\n",
    "    full_data = df[df['model_type'] == 'Full_FineTune'].copy()\n",
    "    \n",
    "    if peft_data.empty:\n",
    "        print(\"No PEFT data available for interactive analysis.\")\n",
    "        return\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('LORA Parameter Efficiency', 'Performance vs Efficiency', \n",
    "                       'Training Dynamics', 'Cross-Method Comparison'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": True}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # 1. LORA Parameter Efficiency\n",
    "    if 'lora_rank' in peft_data.columns and 'parameter_efficiency_ratio' in peft_data.columns:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=peft_data['lora_rank'],\n",
    "                y=peft_data['parameter_efficiency_ratio'] * 100,\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=10,\n",
    "                    color=peft_data['final_wer'],\n",
    "                    colorscale='RdYlGn_r',\n",
    "                    showscale=True,\n",
    "                    colorbar=dict(title=\"WER (%)\", x=0.45)\n",
    "                ),\n",
    "                text=peft_data['dialect'],\n",
    "                hovertemplate='<b>%{text}</b><br>Rank: %{x}<br>Param Efficiency: %{y:.3f}%<br>WER: %{marker.color:.1f}%<extra></extra>',\n",
    "                name='LORA Experiments'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # 2. Performance vs Efficiency\n",
    "    if 'param_efficiency' in df.columns and 'memory_efficiency' in df.columns:\n",
    "        for method in df['model_type'].unique():\n",
    "            method_data = df[df['model_type'] == method]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=method_data['param_efficiency'],\n",
    "                    y=method_data['memory_efficiency'],\n",
    "                    mode='markers',\n",
    "                    marker=dict(size=8),\n",
    "                    name=method,\n",
    "                    text=method_data['dialect'],\n",
    "                    hovertemplate='<b>%{text}</b><br>Param Efficiency: %{x:.2f}<br>Memory Efficiency: %{y:.2f}<extra></extra>'\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "    \n",
    "    # 3. Training Dynamics\n",
    "    if 'training_time_hours' in df.columns:\n",
    "        # Training time comparison\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=peft_data['dialect'],\n",
    "                y=peft_data['training_time_hours'],\n",
    "                name='PEFT Training Time',\n",
    "                marker_color='lightblue',\n",
    "                yaxis='y3'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Performance overlay\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=peft_data['dialect'],\n",
    "                y=peft_data['final_wer'],\n",
    "                mode='markers+lines',\n",
    "                name='PEFT WER',\n",
    "                marker=dict(color='red', size=8),\n",
    "                yaxis='y4'\n",
    "            ),\n",
    "            row=2, col=1, secondary_y=True\n",
    "        )\n",
    "    \n",
    "    # 4. Cross-Method Comparison\n",
    "    if not full_data.empty:\n",
    "        # Create comparison data\n",
    "        comparison_data = []\n",
    "        for dialect in df['dialect'].unique():\n",
    "            peft_perf = peft_data[peft_data['dialect'] == dialect]['final_wer'].mean()\n",
    "            full_perf = full_data[full_data['dialect'] == dialect]['final_wer'].mean()\n",
    "            \n",
    "            if not pd.isna(peft_perf) and not pd.isna(full_perf):\n",
    "                comparison_data.append({\n",
    "                    'dialect': dialect,\n",
    "                    'PEFT_WER': peft_perf,\n",
    "                    'FullFT_WER': full_perf,\n",
    "                    'improvement': full_perf - peft_perf\n",
    "                })\n",
    "        \n",
    "        if comparison_data:\n",
    "            comp_df = pd.DataFrame(comparison_data)\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=comp_df['dialect'],\n",
    "                    y=comp_df['PEFT_WER'],\n",
    "                    name='PEFT LoRA',\n",
    "                    marker_color='lightgreen'\n",
    "                ),\n",
    "                row=2, col=2\n",
    "            )\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=comp_df['dialect'],\n",
    "                    y=comp_df['FullFT_WER'],\n",
    "                    name='Full Fine-tune',\n",
    "                    marker_color='lightcoral'\n",
    "                ),\n",
    "                row=2, col=2\n",
    "            )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title_text=\"Interactive LORA Effectiveness Dashboard\",\n",
    "        title_font_size=20,\n",
    "        height=800,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\"LoRA Rank\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Parameter Efficiency (%)\", row=1, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Parameter Efficiency\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Memory Efficiency\", row=1, col=2)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Dialect\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Training Time (hours)\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"WER (%)\", row=2, col=1, secondary_y=True)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Dialect\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"WER (%)\", row=2, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Create effectiveness radar chart\n",
    "    if not peft_data.empty:\n",
    "        create_lora_effectiveness_radar(peft_data)\n",
    "\n",
    "def create_lora_effectiveness_radar(peft_data):\n",
    "    \"\"\"Create radar chart for LORA effectiveness metrics.\"\"\"\n",
    "    \n",
    "    effectiveness_metrics = []\n",
    "    for metric in ['param_efficiency', 'memory_efficiency', 'time_efficiency', \n",
    "                  'lora_effectiveness', 'rank_efficiency']:\n",
    "        if metric in peft_data.columns:\n",
    "            effectiveness_metrics.append(metric)\n",
    "    \n",
    "    if len(effectiveness_metrics) < 3:\n",
    "        print(\"Insufficient effectiveness metrics for radar chart.\")\n",
    "        return\n",
    "    \n",
    "    # Calculate average metrics across all experiments\n",
    "    avg_metrics = peft_data[effectiveness_metrics].mean()\n",
    "    \n",
    "    # Normalize to 0-1 scale\n",
    "    normalized_metrics = (avg_metrics - avg_metrics.min()) / (avg_metrics.max() - avg_metrics.min())\n",
    "    \n",
    "    # Create radar chart\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=normalized_metrics.values,\n",
    "        theta=[metric.replace('_', ' ').title() for metric in effectiveness_metrics],\n",
    "        fill='toself',\n",
    "        name='Average LORA Effectiveness',\n",
    "        line_color='blue'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, 1]\n",
    "            )),\n",
    "        title=\"LORA Effectiveness Radar Chart\",\n",
    "        title_font_size=16\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Create interactive dashboard\n",
    "if not metrics_df.empty:\n",
    "    create_interactive_lora_dashboard(metrics_df)\n",
    "else:\n",
    "    print(\"No data available for interactive dashboard.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f2fa35",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Method Comparison\n",
    "Statistical comparison between PEFT LoRA and Full Fine-tuning with significance testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f9257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_method_comparison(df):\n",
    "    \"\"\"Perform comprehensive statistical comparison between methods.\"\"\"\n",
    "    if df.empty or 'model_type' not in df.columns:\n",
    "        print(\"Insufficient data for method comparison.\")\n",
    "        return\n",
    "    \n",
    "    peft_data = df[df['model_type'] == 'PEFT_LoRA']\n",
    "    full_data = df[df['model_type'] == 'Full_FineTune']\n",
    "    \n",
    "    if peft_data.empty or full_data.empty:\n",
    "        print(\"Need both PEFT and Full Fine-tuning data for comparison.\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"COMPREHENSIVE METHOD COMPARISON ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Performance comparison\n",
    "    print(f\"\n",
    "1. PERFORMANCE COMPARISON\")\n",
    "    print(f\"{'-'*40}\")\n",
    "    \n",
    "    metrics_to_compare = ['final_wer', 'final_cer', 'training_time_hours', \n",
    "                         'peak_memory_gb', 'trainable_params_millions']\n",
    "    \n",
    "    comparison_results = {}\n",
    "    \n",
    "    for metric in metrics_to_compare:\n",
    "        if metric in peft_data.columns and metric in full_data.columns:\n",
    "            peft_values = peft_data[metric].dropna()\n",
    "            full_values = full_data[metric].dropna()\n",
    "            \n",
    "            if len(peft_values) > 0 and len(full_values) > 0:\n",
    "                # Calculate statistics\n",
    "                peft_mean = peft_values.mean()\n",
    "                full_mean = full_values.mean()\n",
    "                peft_std = peft_values.std()\n",
    "                full_std = full_values.std()\n",
    "                \n",
    "                # Statistical test (if sufficient samples)\n",
    "                if len(peft_values) > 1 and len(full_values) > 1:\n",
    "                    t_stat, p_value = stats.ttest_ind(peft_values, full_values)\n",
    "                    significant = p_value < 0.05\n",
    "                else:\n",
    "                    t_stat, p_value, significant = None, None, False\n",
    "                \n",
    "                # Calculate improvement\n",
    "                if metric in ['final_wer', 'final_cer']:  # Lower is better\n",
    "                    improvement = ((full_mean - peft_mean) / full_mean) * 100\n",
    "                else:  # Higher is better for time/memory efficiency\n",
    "                    improvement = ((peft_mean - full_mean) / full_mean) * 100\n",
    "                \n",
    "                comparison_results[metric] = {\n",
    "                    'peft_mean': peft_mean,\n",
    "                    'full_mean': full_mean,\n",
    "                    'peft_std': peft_std,\n",
    "                    'full_std': full_std,\n",
    "                    'improvement_pct': improvement,\n",
    "                    't_stat': t_stat,\n",
    "                    'p_value': p_value,\n",
    "                    'significant': significant\n",
    "                }\n",
    "                \n",
    "                print(f\"\n",
    "{metric.replace('_', ' ').title()}:\")\n",
    "                print(f\"  PEFT LoRA:     {peft_mean:.3f} ± {peft_std:.3f}\")\n",
    "                print(f\"  Full Finetune: {full_mean:.3f} ± {full_std:.3f}\")\n",
    "                print(f\"  Improvement:   {improvement:+.1f}%\")\n",
    "                if significant:\n",
    "                    print(f\"  Statistical significance: ✓ (p={p_value:.3f})\")\n",
    "                else:\n",
    "                    print(f\"  Statistical significance: ✗ (p={p_value:.3f})\" if p_value else \"  Statistical significance: N/A\")\n",
    "    \n",
    "    # Efficiency analysis\n",
    "    print(f\"\n",
    "2. EFFICIENCY ANALYSIS\")\n",
    "    print(f\"{'-'*40}\")\n",
    "    \n",
    "    efficiency_metrics = ['param_efficiency', 'memory_efficiency', 'time_efficiency', 'composite_efficiency']\n",
    "    \n",
    "    for metric in efficiency_metrics:\n",
    "        if metric in peft_data.columns and metric in full_data.columns:\n",
    "            peft_eff = peft_data[metric].mean()\n",
    "            full_eff = full_data[metric].mean()\n",
    "            efficiency_gain = ((peft_eff - full_eff) / full_eff) * 100\n",
    "            \n",
    "            print(f\"\n",
    "{metric.replace('_', ' ').title()}:\")\n",
    "            print(f\"  PEFT LoRA:     {peft_eff:.3f}\")\n",
    "            print(f\"  Full Finetune: {full_eff:.3f}\")\n",
    "            print(f\"  Efficiency Gain: {efficiency_gain:+.1f}%\")\n",
    "    \n",
    "    # Resource utilization\n",
    "    print(f\"\n",
    "3. RESOURCE UTILIZATION\")\n",
    "    print(f\"{'-'*40}\")\n",
    "    \n",
    "    if 'trainable_percentage' in peft_data.columns:\n",
    "        avg_peft_params = peft_data['trainable_percentage'].mean()\n",
    "        avg_full_params = full_data['trainable_percentage'].mean() if 'trainable_percentage' in full_data.columns else 100\n",
    "        \n",
    "        print(f\"\n",
    "Trainable Parameters:\")\n",
    "        print(f\"  PEFT LoRA:     {avg_peft_params:.2f}% of base model\")\n",
    "        print(f\"  Full Finetune: {avg_full_params:.2f}% of base model\")\n",
    "        print(f\"  Parameter Reduction: {100 - avg_peft_params:.1f}%\")\n",
    "    \n",
    "    if 'peak_memory_gb' in peft_data.columns and 'peak_memory_gb' in full_data.columns:\n",
    "        memory_reduction = ((full_data['peak_memory_gb'].mean() - peft_data['peak_memory_gb'].mean()) / \n",
    "                           full_data['peak_memory_gb'].mean()) * 100\n",
    "        print(f\"\n",
    "Memory Usage Reduction: {memory_reduction:.1f}%\")\n",
    "    \n",
    "    # Per-dialect analysis\n",
    "    print(f\"\n",
    "4. PER-DIALECT PERFORMANCE\")\n",
    "    print(f\"{'-'*40}\")\n",
    "    \n",
    "    dialect_comparison = []\n",
    "    for dialect in df['dialect'].unique():\n",
    "        peft_dialect = peft_data[peft_data['dialect'] == dialect]['final_wer']\n",
    "        full_dialect = full_data[full_data['dialect'] == dialect]['final_wer']\n",
    "        \n",
    "        if len(peft_dialect) > 0 and len(full_dialect) > 0:\n",
    "            peft_wer = peft_dialect.mean()\n",
    "            full_wer = full_dialect.mean()\n",
    "            improvement = ((full_wer - peft_wer) / full_wer) * 100\n",
    "            \n",
    "            dialect_comparison.append({\n",
    "                'dialect': dialect,\n",
    "                'peft_wer': peft_wer,\n",
    "                'full_wer': full_wer,\n",
    "                'improvement': improvement\n",
    "            })\n",
    "            \n",
    "            print(f\"\n",
    "{dialect.capitalize()}:\")\n",
    "            print(f\"  PEFT WER:      {peft_wer:.2f}%\")\n",
    "            print(f\"  Full FT WER:   {full_wer:.2f}%\")\n",
    "            print(f\"  Improvement:   {improvement:+.1f}%\")\n",
    "    \n",
    "    # Create visualization\n",
    "    create_comparison_visualizations(comparison_results, dialect_comparison)\n",
    "    \n",
    "    return comparison_results, dialect_comparison\n",
    "\n",
    "def create_comparison_visualizations(comparison_results, dialect_comparison):\n",
    "    \"\"\"Create visualizations for method comparison.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Method Comparison Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Performance metrics comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    performance_metrics = ['final_wer', 'final_cer']\n",
    "    peft_perf = [comparison_results[m]['peft_mean'] for m in performance_metrics if m in comparison_results]\n",
    "    full_perf = [comparison_results[m]['full_mean'] for m in performance_metrics if m in comparison_results]\n",
    "    \n",
    "    if peft_perf and full_perf:\n",
    "        x = np.arange(len(peft_perf))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax1.bar(x - width/2, peft_perf, width, label='PEFT LoRA', alpha=0.7)\n",
    "        ax1.bar(x + width/2, full_perf, width, label='Full Fine-tune', alpha=0.7)\n",
    "        ax1.set_xlabel('Metrics')\n",
    "        ax1.set_ylabel('Error Rate (%)')\n",
    "        ax1.set_title('Performance Comparison')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels([m.replace('_', ' ').title() for m in performance_metrics if m in comparison_results])\n",
    "        ax1.legend()\n",
    "    \n",
    "    # 2. Resource usage comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    resource_metrics = ['training_time_hours', 'peak_memory_gb']\n",
    "    peft_res = [comparison_results[m]['peft_mean'] for m in resource_metrics if m in comparison_results]\n",
    "    full_res = [comparison_results[m]['full_mean'] for m in resource_metrics if m in comparison_results]\n",
    "    \n",
    "    if peft_res and full_res:\n",
    "        x = np.arange(len(peft_res))\n",
    "        \n",
    "        ax2.bar(x - width/2, peft_res, width, label='PEFT LoRA', alpha=0.7)\n",
    "        ax2.bar(x + width/2, full_res, width, label='Full Fine-tune', alpha=0.7)\n",
    "        ax2.set_xlabel('Metrics')\n",
    "        ax2.set_ylabel('Resource Usage')\n",
    "        ax2.set_title('Resource Usage Comparison')\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels([m.replace('_', ' ').title() for m in resource_metrics if m in comparison_results])\n",
    "        ax2.legend()\n",
    "    \n",
    "    # 3. Improvement percentages\n",
    "    ax3 = axes[1, 0]\n",
    "    improvements = [comparison_results[m]['improvement_pct'] for m in comparison_results]\n",
    "    metrics_names = [m.replace('_', ' ').title() for m in comparison_results.keys()]\n",
    "    \n",
    "    colors = ['green' if imp > 0 else 'red' for imp in improvements]\n",
    "    ax3.barh(metrics_names, improvements, color=colors, alpha=0.7)\n",
    "    ax3.set_xlabel('Improvement (%)')\n",
    "    ax3.set_title('PEFT vs Full Fine-tune Improvements')\n",
    "    ax3.axvline(x=0, color='black', linestyle='-', alpha=0.5)\n",
    "    \n",
    "    # 4. Per-dialect comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    if dialect_comparison:\n",
    "        dialects = [d['dialect'] for d in dialect_comparison]\n",
    "        peft_wers = [d['peft_wer'] for d in dialect_comparison]\n",
    "        full_wers = [d['full_wer'] for d in dialect_comparison]\n",
    "        \n",
    "        x = np.arange(len(dialects))\n",
    "        ax4.bar(x - width/2, peft_wers, width, label='PEFT LoRA', alpha=0.7)\n",
    "        ax4.bar(x + width/2, full_wers, width, label='Full Fine-tune', alpha=0.7)\n",
    "        ax4.set_xlabel('Dialect')\n",
    "        ax4.set_ylabel('WER (%)')\n",
    "        ax4.set_title('Per-Dialect Performance')\n",
    "        ax4.set_xticks(x)\n",
    "        ax4.set_xticklabels([d.capitalize() for d in dialects], rotation=45)\n",
    "        ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run comprehensive comparison\n",
    "if not metrics_df.empty:\n",
    "    comparison_results, dialect_comparison = comprehensive_method_comparison(metrics_df)\n",
    "else:\n",
    "    print(\"No data available for method comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e232b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_comparison(df):\n",
    "    \"\"\"Plot performance comparison between PEFT and Full Fine-tuning.\"\"\"\n",
    "    if df.empty or 'final_wer' not in df.columns:\n",
    "        print(\"No WER data available for plotting.\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('PEFT vs Full Fine-tuning: Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. WER by Dialect and Method\n",
    "    if 'dialect' in df.columns and 'model_type' in df.columns:\n",
    "        sns.barplot(data=df, x='dialect', y='final_wer', hue='model_type', ax=axes[0,0])\n",
    "        axes[0,0].set_title('Word Error Rate (WER) by Dialect')\n",
    "        axes[0,0].set_ylabel('WER (%)')\n",
    "        axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Training Loss by Method\n",
    "    if 'final_loss' in df.columns and 'model_type' in df.columns:\n",
    "        sns.boxplot(data=df, x='model_type', y='final_loss', ax=axes[0,1])\n",
    "        axes[0,1].set_title('Final Training Loss Distribution')\n",
    "        axes[0,1].set_ylabel('Training Loss')\n",
    "    \n",
    "    # 3. WER vs Training Time\n",
    "    if 'training_time_minutes' in df.columns:\n",
    "        scatter_colors = {'PEFT_LoRA': 'blue', 'Full_FineTune': 'red'}\n",
    "        for model_type, group in df.groupby('model_type'):\n",
    "            color = scatter_colors.get(model_type, 'gray')\n",
    "            axes[1,0].scatter(group['training_time_minutes'], group['final_wer'], \n",
    "                            label=model_type, alpha=0.7, s=80, color=color)\n",
    "        \n",
    "        axes[1,0].set_xlabel('Training Time (minutes)')\n",
    "        axes[1,0].set_ylabel('WER (%)')\n",
    "        axes[1,0].set_title('Performance vs Training Time')\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Average Performance Summary\n",
    "    if 'model_type' in df.columns:\n",
    "        avg_metrics = df.groupby('model_type')[['final_wer', 'final_loss']].mean()\n",
    "        avg_metrics.plot(kind='bar', ax=axes[1,1], rot=45)\n",
    "        axes[1,1].set_title('Average Performance Metrics')\n",
    "        axes[1,1].set_ylabel('Value')\n",
    "        axes[1,1].legend(['WER (%)', 'Training Loss'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate performance plots\n",
    "if not metrics_df.empty:\n",
    "    plot_performance_comparison(metrics_df)\n",
    "else:\n",
    "    print(\"No data available for plotting. Please run training experiments first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b733e1",
   "metadata": {},
   "source": [
    "## 5. Generate Resource Usage Plots\n",
    "Plot resource usage metrics including memory consumption and parameter efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe54e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_resource_usage(df):\n",
    "    \"\"\"Plot resource usage comparison.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"No data available for resource plotting.\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Resource Usage Comparison: PEFT vs Full Fine-tuning', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Memory Usage\n",
    "    if 'peak_memory_gb' in df.columns and 'model_type' in df.columns:\n",
    "        sns.barplot(data=df, x='model_type', y='peak_memory_gb', ax=axes[0,0])\n",
    "        axes[0,0].set_title('Peak GPU Memory Usage')\n",
    "        axes[0,0].set_ylabel('Memory (GB)')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, bar in enumerate(axes[0,0].patches):\n",
    "            height = bar.get_height()\n",
    "            axes[0,0].text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                          f'{height:.1f}GB', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Trainable Parameters\n",
    "    if 'trainable_params_millions' in df.columns and 'model_type' in df.columns:\n",
    "        sns.barplot(data=df, x='model_type', y='trainable_params_millions', ax=axes[0,1])\n",
    "        axes[0,1].set_title('Trainable Parameters')\n",
    "        axes[0,1].set_ylabel('Parameters (Millions)')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, bar in enumerate(axes[0,1].patches):\n",
    "            height = bar.get_height()\n",
    "            axes[0,1].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                          f'{height:.1f}M', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Training Time by Dialect\n",
    "    if 'training_time_minutes' in df.columns and 'dialect' in df.columns:\n",
    "        sns.barplot(data=df, x='dialect', y='training_time_minutes', hue='model_type', ax=axes[1,0])\n",
    "        axes[1,0].set_title('Training Time by Dialect')\n",
    "        axes[1,0].set_ylabel('Time (minutes)')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Efficiency Score (Performance per GB)\n",
    "    if 'efficiency_score' in df.columns and 'model_type' in df.columns:\n",
    "        sns.boxplot(data=df, x='model_type', y='efficiency_score', ax=axes[1,1])\n",
    "        axes[1,1].set_title('Efficiency Score\\n(Performance per GB Memory)')\n",
    "        axes[1,1].set_ylabel('Efficiency Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate resource usage plots\n",
    "if not metrics_df.empty:\n",
    "    plot_resource_usage(metrics_df)\n",
    "else:\n",
    "    print(\"No data available for plotting. Please run training experiments first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82749847",
   "metadata": {},
   "source": [
    "## 6. Create Efficiency Summary Table\n",
    "Generate a summary table comparing PEFT vs Full Fine-tuning efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d53d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_efficiency_summary(df):\n",
    "    \"\"\"Create efficiency summary table.\"\"\"\n",
    "    if df.empty or 'model_type' not in df.columns:\n",
    "        print(\"No data available for efficiency summary.\")\n",
    "        return\n",
    "    \n",
    "    # Group by model type and calculate averages\n",
    "    summary_cols = ['final_wer', 'training_time_minutes', 'peak_memory_gb', \n",
    "                   'trainable_params_millions', 'total_params_millions']\n",
    "    \n",
    "    available_cols = [col for col in summary_cols if col in df.columns]\n",
    "    \n",
    "    if not available_cols:\n",
    "        print(\"No relevant columns available for summary.\")\n",
    "        return\n",
    "    \n",
    "    summary = df.groupby('model_type')[available_cols].mean().round(2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EFFICIENCY SUMMARY: PEFT LoRA vs Full Fine-tuning\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Display the summary table\n",
    "    print(summary.to_string())\n",
    "    \n",
    "    # Calculate improvement percentages\n",
    "    if len(summary) >= 2:\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        print(\"PEFT LoRA IMPROVEMENTS OVER FULL FINE-TUNING:\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        peft_row = summary.loc[summary.index.str.contains('PEFT', case=False, na=False)]\n",
    "        full_row = summary.loc[summary.index.str.contains('Full', case=False, na=False)]\n",
    "        \n",
    "        if not peft_row.empty and not full_row.empty:\n",
    "            peft_values = peft_row.iloc[0]\n",
    "            full_values = full_row.iloc[0]\n",
    "            \n",
    "            # Calculate improvements (negative = better for WER, time, memory)\n",
    "            improvements = {\n",
    "                'WER': ((peft_values.get('final_wer', 0) - full_values.get('final_wer', 0)) / full_values.get('final_wer', 1)) * 100,\n",
    "                'Training Time': ((peft_values.get('training_time_minutes', 0) - full_values.get('training_time_minutes', 0)) / full_values.get('training_time_minutes', 1)) * 100,\n",
    "                'Memory Usage': ((peft_values.get('peak_memory_gb', 0) - full_values.get('peak_memory_gb', 0)) / full_values.get('peak_memory_gb', 1)) * 100,\n",
    "                'Trainable Parameters': ((peft_values.get('trainable_params_millions', 0) - full_values.get('trainable_params_millions', 0)) / full_values.get('trainable_params_millions', 1)) * 100\n",
    "            }\n",
    "            \n",
    "            for metric, improvement in improvements.items():\n",
    "                if not np.isnan(improvement) and abs(improvement) < 1000:  # Sanity check\n",
    "                    sign = \"↓\" if improvement < 0 else \"↑\"\n",
    "                    print(f\"{metric:20}: {improvement:+6.1f}% {sign}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Generate efficiency summary\n",
    "if not metrics_df.empty:\n",
    "    create_efficiency_summary(metrics_df)\n",
    "else:\n",
    "    print(\"No data available for summary. Please run training experiments first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f344e01",
   "metadata": {},
   "source": [
    "## 7. Generate Model Performance Comparison Charts\n",
    "Create detailed comparison charts showing performance across different dialects and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ecf5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detailed_comparison(df):\n",
    "    \"\"\"Create detailed comparison charts.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"No data available for detailed comparison.\")\n",
    "        return\n",
    "    \n",
    "    # Create a comprehensive comparison figure\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Performance by Dialect (Large subplot)\n",
    "    ax1 = plt.subplot(3, 3, (1, 4))\n",
    "    if 'dialect' in df.columns and 'final_wer' in df.columns:\n",
    "        dialect_performance = df.groupby(['dialect', 'model_type'])['final_wer'].mean().unstack()\n",
    "        dialect_performance.plot(kind='bar', ax=ax1, rot=45, width=0.8)\n",
    "        ax1.set_title('WER Performance by Dialect', fontsize=14, fontweight='bold')\n",
    "        ax1.set_ylabel('Word Error Rate (%)')\n",
    "        ax1.legend(title='Method')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Memory vs Performance Scatter\n",
    "    ax2 = plt.subplot(3, 3, 3)\n",
    "    if 'peak_memory_gb' in df.columns and 'final_wer' in df.columns:\n",
    "        for model_type, group in df.groupby('model_type'):\n",
    "            ax2.scatter(group['peak_memory_gb'], group['final_wer'], \n",
    "                       label=model_type, alpha=0.7, s=80)\n",
    "        ax2.set_xlabel('Memory (GB)')\n",
    "        ax2.set_ylabel('WER (%)')\n",
    "        ax2.set_title('Memory vs Performance')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Parameter Efficiency\n",
    "    ax3 = plt.subplot(3, 3, 6)\n",
    "    if 'trainable_percentage' in df.columns and 'model_type' in df.columns:\n",
    "        sns.barplot(data=df, x='model_type', y='trainable_percentage', ax=ax3)\n",
    "        ax3.set_title('Parameter Efficiency')\n",
    "        ax3.set_ylabel('Trainable Parameters (%)')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Training Time Distribution\n",
    "    ax4 = plt.subplot(3, 3, 7)\n",
    "    if 'training_time_minutes' in df.columns and 'model_type' in df.columns:\n",
    "        df.boxplot(column='training_time_minutes', by='model_type', ax=ax4)\n",
    "        ax4.set_title('Training Time Distribution')\n",
    "        ax4.set_ylabel('Time (minutes)')\n",
    "        ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 5. Overall Efficiency Radar Chart (if we have multiple metrics)\n",
    "    ax5 = plt.subplot(3, 3, 8)\n",
    "    if 'model_type' in df.columns and len(df['model_type'].unique()) >= 2:\n",
    "        # Create a simple efficiency comparison\n",
    "        metrics = ['final_wer', 'training_time_minutes', 'peak_memory_gb']\n",
    "        available_metrics = [m for m in metrics if m in df.columns]\n",
    "        \n",
    "        if available_metrics:\n",
    "            efficiency_data = df.groupby('model_type')[available_metrics].mean()\n",
    "            # Normalize metrics (lower is better for all these metrics)\n",
    "            normalized_data = 100 / efficiency_data  # Inverse so higher is better\n",
    "            \n",
    "            normalized_data.plot(kind='bar', ax=ax5, rot=45)\n",
    "            ax5.set_title('Efficiency Comparison\\n(Higher = Better)')\n",
    "            ax5.set_ylabel('Efficiency Score')\n",
    "            ax5.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # 6. Summary Statistics\n",
    "    ax6 = plt.subplot(3, 3, 9)\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    # Create summary text\n",
    "    summary_text = \"EXPERIMENT SUMMARY\\n\\n\"\n",
    "    if not df.empty:\n",
    "        summary_text += f\"Total Experiments: {len(df)}\\n\"\n",
    "        if 'dialect' in df.columns:\n",
    "            summary_text += f\"Dialects: {', '.join(df['dialect'].unique())}\\n\"\n",
    "        if 'model_type' in df.columns:\n",
    "            summary_text += f\"Methods: {', '.join(df['model_type'].unique())}\\n\\n\"\n",
    "        \n",
    "        # Best performance\n",
    "        if 'final_wer' in df.columns:\n",
    "            best_wer = df.loc[df['final_wer'].idxmin()]\n",
    "            summary_text += f\"Best WER: {best_wer['final_wer']:.2f}%\\n\"\n",
    "            summary_text += f\"Method: {best_wer.get('model_type', 'N/A')}\\n\"\n",
    "            summary_text += f\"Dialect: {best_wer.get('dialect', 'N/A')}\\n\"\n",
    "    \n",
    "    ax6.text(0.1, 0.9, summary_text, transform=ax6.transAxes, \n",
    "             fontsize=11, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    plt.suptitle('Comprehensive PEFT vs Full Fine-tuning Analysis', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate detailed comparison\n",
    "if not metrics_df.empty:\n",
    "    plot_detailed_comparison(metrics_df)\n",
    "else:\n",
    "    print(\"No data available for plotting. Please run training experiments first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2bc542",
   "metadata": {},
   "source": [
    "## 8. Save Analysis Results\n",
    "Save the analysis results and plots for documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466525a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_analysis_results(df, output_dir=\"./analysis_results\"):\n",
    "    \"\"\"Save analysis results to files.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save processed data\n",
    "    csv_path = output_path / \"training_metrics_processed.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Processed data saved to: {csv_path}\")\n",
    "    \n",
    "    # Save summary statistics\n",
    "    if 'model_type' in df.columns:\n",
    "        summary_cols = ['final_wer', 'training_time_minutes', 'peak_memory_gb', \n",
    "                       'trainable_params_millions']\n",
    "        available_cols = [col for col in summary_cols if col in df.columns]\n",
    "        \n",
    "        if available_cols:\n",
    "            summary = df.groupby('model_type')[available_cols].agg(['mean', 'std']).round(3)\n",
    "            summary_path = output_path / \"efficiency_summary.csv\"\n",
    "            summary.to_csv(summary_path)\n",
    "            print(f\"Summary statistics saved to: {summary_path}\")\n",
    "    \n",
    "    print(f\"\\nAll analysis results saved to: {output_path}\")\n",
    "\n",
    "# Save results\n",
    "if not metrics_df.empty:\n",
    "    save_analysis_results(metrics_df)\n",
    "    \n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"\\nTo generate this analysis:\")\n",
    "print(\"1. Run training experiments with the updated script\")\n",
    "print(\"2. The script automatically saves metrics_*.json files\")\n",
    "print(\"3. This notebook loads and analyzes all available metrics\")\n",
    "print(\"4. Comparison plots show PEFT vs Full Fine-tuning efficiency\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f37836a",
   "metadata": {},
   "source": [
    "## 9. Export Comprehensive Analysis Report\n",
    "Generate and export a comprehensive analysis report with all findings and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700fece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comprehensive_report(df, output_dir=\"./analysis_results\"):\n",
    "    \"\"\"Generate a comprehensive analysis report with executive summary.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"No data available for report generation.\")\n",
    "        return\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Generate report content\n",
    "    report_content = []\n",
    "    \n",
    "    # Executive Summary\n",
    "    report_content.append(\"# Arabic Dialect PEFT Training Analysis Report\")\n",
    "    report_content.append(\"=\" * 60)\n",
    "    report_content.append(\"\")\n",
    "    report_content.append(\"## Executive Summary\")\n",
    "    report_content.append(\"\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_experiments = len(df)\n",
    "    methods = df['model_type'].value_counts().to_dict() if 'model_type' in df.columns else {}\n",
    "    dialects = df['dialect'].value_counts().to_dict() if 'dialect' in df.columns else {}\n",
    "    \n",
    "    report_content.append(f\"**Total Experiments Analyzed:** {total_experiments}\")\n",
    "    report_content.append(f\"**Methods Compared:** {', '.join(methods.keys())}\")\n",
    "    report_content.append(f\"**Dialects Tested:** {', '.join(dialects.keys())}\")\n",
    "    report_content.append(\"\")\n",
    "    \n",
    "    # Performance summary\n",
    "    if 'final_wer' in df.columns:\n",
    "        best_wer = df['final_wer'].min()\n",
    "        avg_wer = df['final_wer'].mean()\n",
    "        worst_wer = df['final_wer'].max()\n",
    "        \n",
    "        report_content.append(\"### Performance Overview\")\n",
    "        report_content.append(f\"- **Best WER achieved:** {best_wer:.2f}%\")\n",
    "        report_content.append(f\"- **Average WER:** {avg_wer:.2f}%\")\n",
    "        report_content.append(f\"- **WER range:** {best_wer:.2f}% - {worst_wer:.2f}%\")\n",
    "        report_content.append(\"\")\n",
    "    \n",
    "    # Method comparison\n",
    "    if 'model_type' in df.columns and len(df['model_type'].unique()) > 1:\n",
    "        report_content.append(\"### Method Comparison Summary\")\n",
    "        \n",
    "        peft_data = df[df['model_type'] == 'PEFT_LoRA']\n",
    "        full_data = df[df['model_type'] == 'Full_FineTune']\n",
    "        \n",
    "        if not peft_data.empty and not full_data.empty:\n",
    "            peft_wer = peft_data['final_wer'].mean()\n",
    "            full_wer = full_data['final_wer'].mean()\n",
    "            wer_improvement = ((full_wer - peft_wer) / full_wer) * 100\n",
    "            \n",
    "            report_content.append(f\"- **PEFT LoRA average WER:** {peft_wer:.2f}%\")\n",
    "            report_content.append(f\"- **Full Fine-tune average WER:** {full_wer:.2f}%\")\n",
    "            report_content.append(f\"- **WER improvement:** {wer_improvement:+.1f}%\")\n",
    "            \n",
    "            # Resource efficiency\n",
    "            if 'trainable_percentage' in peft_data.columns:\n",
    "                avg_params = peft_data['trainable_percentage'].mean()\n",
    "                report_content.append(f\"- **Parameter reduction:** {100-avg_params:.1f}%\")\n",
    "            \n",
    "            if 'training_time_hours' in df.columns:\n",
    "                peft_time = peft_data['training_time_hours'].mean()\n",
    "                full_time = full_data['training_time_hours'].mean()\n",
    "                time_saving = ((full_time - peft_time) / full_time) * 100\n",
    "                report_content.append(f\"- **Training time saving:** {time_saving:.1f}%\")\n",
    "            \n",
    "            report_content.append(\"\")\n",
    "    \n",
    "    # Key findings\n",
    "    report_content.append(\"### Key Findings\")\n",
    "    \n",
    "    # Best performing combinations\n",
    "    if 'dialect' in df.columns and 'model_type' in df.columns:\n",
    "        best_combination = df.loc[df['final_wer'].idxmin()]\n",
    "        report_content.append(f\"- **Best performing combination:** {best_combination['model_type']} on {best_combination['dialect']} dialect\")\n",
    "        \n",
    "        # Dialect-specific analysis\n",
    "        dialect_performance = df.groupby('dialect')['final_wer'].mean().sort_values()\n",
    "        best_dialect = dialect_performance.index[0]\n",
    "        worst_dialect = dialect_performance.index[-1]\n",
    "        \n",
    "        report_content.append(f\"- **Easiest dialect:** {best_dialect} (avg WER: {dialect_performance[best_dialect]:.2f}%)\")\n",
    "        report_content.append(f\"- **Most challenging dialect:** {worst_dialect} (avg WER: {dialect_performance[worst_dialect]:.2f}%)\")\n",
    "    \n",
    "    # LORA-specific insights\n",
    "    peft_data = df[df['model_type'] == 'PEFT_LoRA']\n",
    "    if not peft_data.empty:\n",
    "        report_content.append(\"\")\n",
    "        report_content.append(\"### LORA-Specific Insights\")\n",
    "        \n",
    "        if 'lora_rank' in peft_data.columns:\n",
    "            rank_performance = peft_data.groupby('lora_rank')['final_wer'].mean()\n",
    "            best_rank = rank_performance.idxmin()\n",
    "            report_content.append(f\"- **Optimal LoRA rank:** {best_rank} (WER: {rank_performance[best_rank]:.2f}%)\")\n",
    "        \n",
    "        if 'parameter_efficiency_ratio' in peft_data.columns:\n",
    "            avg_efficiency = peft_data['parameter_efficiency_ratio'].mean()\n",
    "            report_content.append(f\"- **Average parameter efficiency:** {avg_efficiency:.4f} ({avg_efficiency*100:.2f}% of base model)\")\n",
    "        \n",
    "        if 'lora_effectiveness' in peft_data.columns:\n",
    "            avg_effectiveness = peft_data['lora_effectiveness'].mean()\n",
    "            report_content.append(f\"- **Average LoRA effectiveness score:** {avg_effectiveness:.2f}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    report_content.append(\"\")\n",
    "    report_content.append(\"### Recommendations\")\n",
    "    \n",
    "    if not peft_data.empty and not full_data.empty:\n",
    "        if peft_wer <= full_wer * 1.05:  # Within 5% of full fine-tuning\n",
    "            report_content.append(\"- **Recommended approach:** PEFT LoRA for production use\")\n",
    "            report_content.append(\"  - Achieves comparable performance to full fine-tuning\")\n",
    "            report_content.append(\"  - Significantly reduces computational requirements\")\n",
    "            report_content.append(\"  - Faster training and deployment\")\n",
    "        else:\n",
    "            report_content.append(\"- **Consider hybrid approach:** PEFT for prototyping, full fine-tuning for production\")\n",
    "            report_content.append(\"  - Use PEFT for rapid experimentation\")\n",
    "            report_content.append(\"  - Full fine-tuning for maximum performance\")\n",
    "    \n",
    "    if 'dialect' in df.columns:\n",
    "        # Dialect-specific recommendations\n",
    "        difficult_dialects = dialect_performance.tail(2).index.tolist()\n",
    "        if difficult_dialects:\n",
    "            report_content.append(f\"- **Focus additional research on:** {', '.join(difficult_dialects)} dialects\")\n",
    "            report_content.append(\"  - Consider dialect-specific data augmentation\")\n",
    "            report_content.append(\"  - Explore cross-dialect transfer learning\")\n",
    "    \n",
    "    # Technical details\n",
    "    report_content.append(\"\")\n",
    "    report_content.append(\"## Detailed Technical Analysis\")\n",
    "    report_content.append(\"\")\n",
    "    \n",
    "    # Statistical summary\n",
    "    if 'model_type' in df.columns:\n",
    "        summary_cols = ['final_wer', 'final_cer', 'training_time_hours', 'peak_memory_gb']\n",
    "        available_cols = [col for col in summary_cols if col in df.columns]\n",
    "        \n",
    "        if available_cols:\n",
    "            summary_stats = df.groupby('model_type')[available_cols].agg(['mean', 'std']).round(3)\n",
    "            \n",
    "            report_content.append(\"### Performance Statistics by Method\")\n",
    "            report_content.append(\"\")\n",
    "            report_content.append(\"```\")\n",
    "            report_content.append(summary_stats.to_string())\n",
    "            report_content.append(\"```\")\n",
    "            report_content.append(\"\")\n",
    "    \n",
    "    # Experimental setup\n",
    "    report_content.append(\"### Experimental Setup\")\n",
    "    if 'base_model' in df.columns:\n",
    "        base_models = df['base_model'].unique()\n",
    "        report_content.append(f\"- **Base Models:** {', '.join(base_models)}\")\n",
    "    \n",
    "    if 'lora_rank' in peft_data.columns:\n",
    "        ranks_tested = sorted(peft_data['lora_rank'].unique())\n",
    "        report_content.append(f\"- **LoRA Ranks Tested:** {ranks_tested}\")\n",
    "    \n",
    "    if 'batch_size' in df.columns:\n",
    "        batch_sizes = sorted(df['batch_size'].unique())\n",
    "        report_content.append(f\"- **Batch Sizes:** {batch_sizes}\")\n",
    "    \n",
    "    # Save report\n",
    "    report_text = \"\\n\".join(report_content)\n",
    "    report_path = output_path / \"comprehensive_analysis_report.md\"\n",
    "    \n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(report_text)\n",
    "    \n",
    "    print(f\"Comprehensive analysis report saved to: {report_path}\")\n",
    "    \n",
    "    # Also save as text file\n",
    "    txt_path = output_path / \"analysis_summary.txt\"\n",
    "    with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(report_text)\n",
    "    \n",
    "    print(f\"Text summary saved to: {txt_path}\")\n",
    "    \n",
    "    # Generate comparison table\n",
    "    if 'model_type' in df.columns and len(df['model_type'].unique()) > 1:\n",
    "        comparison_table = create_comparison_table(df)\n",
    "        table_path = output_path / \"method_comparison_table.csv\"\n",
    "        comparison_table.to_csv(table_path)\n",
    "        print(f\"Comparison table saved to: {table_path}\")\n",
    "    \n",
    "    return report_path\n",
    "\n",
    "def create_comparison_table(df):\n",
    "    \"\"\"Create a detailed comparison table.\"\"\"\n",
    "    if 'model_type' not in df.columns:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Define metrics to compare\n",
    "    metrics = ['final_wer', 'final_cer', 'training_time_hours', 'peak_memory_gb', \n",
    "              'trainable_params_millions', 'trainable_percentage']\n",
    "    \n",
    "    available_metrics = [m for m in metrics if m in df.columns]\n",
    "    \n",
    "    if not available_metrics:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_data = []\n",
    "    \n",
    "    for method in df['model_type'].unique():\n",
    "        method_data = df[df['model_type'] == method]\n",
    "        \n",
    "        row = {'Method': method, 'Experiments': len(method_data)}\n",
    "        \n",
    "        for metric in available_metrics:\n",
    "            values = method_data[metric].dropna()\n",
    "            if len(values) > 0:\n",
    "                row[f'{metric}_mean'] = values.mean()\n",
    "                row[f'{metric}_std'] = values.std()\n",
    "                row[f'{metric}_min'] = values.min()\n",
    "                row[f'{metric}_max'] = values.max()\n",
    "        \n",
    "        comparison_data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "def export_visualizations(output_dir=\"./analysis_results\"):\n",
    "    \"\"\"Export all visualizations as image files.\"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    plots_dir = output_path / \"plots\"\n",
    "    plots_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"Visualization plots will be saved to: {plots_dir}\")\n",
    "    print(\"Note: Run the analysis cells above to generate and save plots.\")\n",
    "    \n",
    "    return plots_dir\n",
    "\n",
    "# Generate comprehensive report\n",
    "if not metrics_df.empty:\n",
    "    print(\"Generating comprehensive analysis report...\")\n",
    "    report_path = generate_comprehensive_report(metrics_df)\n",
    "    plots_dir = export_visualizations()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE ANALYSIS COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"📊 Main report: {report_path}\")\n",
    "    print(f\"📈 Plots directory: {plots_dir}\")\n",
    "    print(f\"📋 CSV data: ./analysis_results/training_metrics_processed.csv\")\n",
    "    print(f\"📝 Summary table: ./analysis_results/method_comparison_table.csv\")\n",
    "    print(\"\\nFiles generated:\")\n",
    "    print(\"- comprehensive_analysis_report.md (Markdown report)\")\n",
    "    print(\"- analysis_summary.txt (Plain text summary)\")  \n",
    "    print(\"- training_metrics_processed.csv (Processed data)\")\n",
    "    print(\"- method_comparison_table.csv (Detailed comparison)\")\n",
    "    print(\"- efficiency_summary.csv (Statistical summary)\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"No data available for comprehensive report generation.\")\n",
    "    print(\"Please run training experiments first to generate metrics data.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
