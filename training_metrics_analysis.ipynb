{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89a190b1",
   "metadata": {},
   "source": [
    "# Arabic Dialect PEFT Training Metrics Analysis\n",
    "\n",
    "This notebook analyzes the training metrics recorded during Arabic dialect fine-tuning experiments.\n",
    "It compares PEFT LoRA vs Full Fine-tuning across different dialects.\n",
    "\n",
    "**Data Source:** Metrics automatically recorded by `dialect_peft_training.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e30e1f",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "Import necessary libraries including matplotlib, pandas, numpy, and seaborn for data visualization and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7965f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1dc647",
   "metadata": {},
   "source": [
    "## 2. Load Training Data\n",
    "Load the recorded training data from JSON files generated during the finetuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b098fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_metrics(base_dir=\"./results\"):\n",
    "    \"\"\"Load all training metrics from JSON files following original repository structure.\"\"\"\n",
    "    \n",
    "    # Look in original repository subdirectories\n",
    "    subdirs = [\"ex_finetune\", \"ex_scratch\", \"ex_peft\", \"ex_comparison_dialectal\"]\n",
    "    all_metrics = []\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        subdir_path = Path(base_dir) / subdir\n",
    "        if subdir_path.exists():\n",
    "            metrics_files = list(subdir_path.glob(\"results_whisper-*.json\"))\n",
    "            \n",
    "            for file_path in metrics_files:\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                    \n",
    "                    # Parse filename for metadata (following original pattern)\n",
    "                    filename = file_path.stem  # results_whisper-small-peft_egyptian_seed42\n",
    "                    parts = filename.split('_')\n",
    "                    \n",
    "                    if len(parts) >= 3:\n",
    "                        # Extract model info from filename\n",
    "                        model_part = parts[1]  # whisper-small-peft or whisper-small-finetune\n",
    "                        dialect = parts[2] if len(parts) > 2 else \"unknown\"\n",
    "                        seed = parts[3].replace(\"seed\", \"\") if len(parts) > 3 and \"seed\" in parts[3] else \"42\"\n",
    "                        \n",
    "                        # Determine method from filename\n",
    "                        if \"peft\" in model_part:\n",
    "                            method = \"PEFT_LoRA\"\n",
    "                        elif \"finetune\" in model_part:\n",
    "                            method = \"Full_FineTune\"\n",
    "                        else:\n",
    "                            method = \"Unknown\"\n",
    "                        \n",
    "                        # Create consistent data structure\n",
    "                        metrics = {\n",
    "                            'experiment_name': filename,\n",
    "                            'dialect': dialect,\n",
    "                            'model_type': method,\n",
    "                            'seed': int(seed),\n",
    "                            'final_wer': data.get('wer', data.get('final_wer', 0)),\n",
    "                            'final_cer': data.get('cer', data.get('final_cer', 0)),\n",
    "                            'training_time_seconds': data.get('training_time_seconds', data.get('training_time', 0)),\n",
    "                            'peak_memory_mb': data.get('peak_memory_mb', 0),\n",
    "                            'trainable_params': data.get('trainable_params', 0),\n",
    "                            'total_params': data.get('total_params', 0),\n",
    "                            'final_loss': data.get('final_loss', 0),\n",
    "                            'source_file': str(file_path)\n",
    "                        }\n",
    "                        \n",
    "                        all_metrics.append(metrics)\n",
    "                        print(f\"Loaded: {file_path} -> {method} {dialect}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    # Also check for any metrics files in main results directory  \n",
    "    main_metrics_files = list(Path(base_dir).glob(\"metrics_*.json\"))\n",
    "    for file_path in main_metrics_files:\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                all_metrics.append(data)\n",
    "                print(f\"Loaded: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    if all_metrics:\n",
    "        df = pd.DataFrame(all_metrics)\n",
    "        print(f\"\n",
    "Loaded {len(df)} experiments\")\n",
    "        print(f\"Methods found: {df['model_type'].unique() if 'model_type' in df.columns else 'N/A'}\")\n",
    "        print(f\"Dialects found: {df['dialect'].unique() if 'dialect' in df.columns else 'N/A'}\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No metrics files found.\")\n",
    "        print(\"Make sure you've run training experiments that save results in:\")\n",
    "        print(\"  - ./results/ex_peft/\")\n",
    "        print(\"  - ./results/ex_finetune/\") \n",
    "        print(\"  - ./results/ex_scratch/\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load the data\n",
    "metrics_df = load_training_metrics()\n",
    "\n",
    "if not metrics_df.empty:\n",
    "    print(\"\\nDataset overview:\")\n",
    "    print(metrics_df.head())\n",
    "    print(f\"\\nColumns: {list(metrics_df.columns)}\")\n",
    "else:\n",
    "    print(\"No data loaded. Please run some training experiments first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644445e2",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Cleaning\n",
    "Clean and preprocess the training data, handle missing values, and format the data for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816993f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_metrics(df):\n",
    "    \"\"\"Clean and preprocess the metrics data.\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    # Convert training time to minutes\n",
    "    if 'training_time_seconds' in df.columns:\n",
    "        df['training_time_minutes'] = df['training_time_seconds'] / 60\n",
    "    \n",
    "    # Convert memory to GB\n",
    "    if 'peak_memory_mb' in df.columns:\n",
    "        df['peak_memory_gb'] = df['peak_memory_mb'] / 1024\n",
    "    \n",
    "    # Convert parameters to millions\n",
    "    if 'total_params' in df.columns:\n",
    "        df['total_params_millions'] = df['total_params'] / 1_000_000\n",
    "    if 'trainable_params' in df.columns:\n",
    "        df['trainable_params_millions'] = df['trainable_params'] / 1_000_000\n",
    "    \n",
    "    # Create efficiency ratio (performance / resources)\n",
    "    if 'final_wer' in df.columns and 'peak_memory_gb' in df.columns:\n",
    "        # Lower WER is better, so use (100 - WER) for efficiency\n",
    "        df['efficiency_score'] = (100 - df['final_wer']) / df['peak_memory_gb']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Preprocess the data\n",
    "if not metrics_df.empty:\n",
    "    metrics_df = preprocess_metrics(metrics_df)\n",
    "    \n",
    "    print(\"Data preprocessing complete!\")\n",
    "    print(f\"\\nSample of processed data:\")\n",
    "    \n",
    "    # Show key metrics\n",
    "    key_cols = ['experiment_name', 'dialect', 'model_type', 'final_wer', \n",
    "                'training_time_minutes', 'peak_memory_gb', 'trainable_params_millions']\n",
    "    \n",
    "    available_cols = [col for col in key_cols if col in metrics_df.columns]\n",
    "    if available_cols:\n",
    "        print(metrics_df[available_cols].head())\n",
    "    \n",
    "    print(f\"\\nDataset shape: {metrics_df.shape}\")\n",
    "    print(f\"Model types: {metrics_df['model_type'].unique() if 'model_type' in metrics_df.columns else 'N/A'}\")\n",
    "    print(f\"Dialects: {metrics_df['dialect'].unique() if 'dialect' in metrics_df.columns else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f77c57",
   "metadata": {},
   "source": [
    "## 4. Generate Training Performance Plots\n",
    "Create plots showing training performance metrics like WER and training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99110d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_comparison(df):\n",
    "    \"\"\"Plot performance comparison between PEFT and Full Fine-tuning.\"\"\"\n",
    "    if df.empty or 'final_wer' not in df.columns:\n",
    "        print(\"No WER data available for plotting.\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('PEFT vs Full Fine-tuning: Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. WER by Dialect and Method\n",
    "    if 'dialect' in df.columns and 'model_type' in df.columns:\n",
    "        sns.barplot(data=df, x='dialect', y='final_wer', hue='model_type', ax=axes[0,0])\n",
    "        axes[0,0].set_title('Word Error Rate (WER) by Dialect')\n",
    "        axes[0,0].set_ylabel('WER (%)')\n",
    "        axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Training Loss by Method\n",
    "    if 'final_loss' in df.columns and 'model_type' in df.columns:\n",
    "        sns.boxplot(data=df, x='model_type', y='final_loss', ax=axes[0,1])\n",
    "        axes[0,1].set_title('Final Training Loss Distribution')\n",
    "        axes[0,1].set_ylabel('Training Loss')\n",
    "    \n",
    "    # 3. WER vs Training Time\n",
    "    if 'training_time_minutes' in df.columns:\n",
    "        scatter_colors = {'PEFT_LoRA': 'blue', 'Full_FineTune': 'red'}\n",
    "        for model_type, group in df.groupby('model_type'):\n",
    "            color = scatter_colors.get(model_type, 'gray')\n",
    "            axes[1,0].scatter(group['training_time_minutes'], group['final_wer'], \n",
    "                            label=model_type, alpha=0.7, s=80, color=color)\n",
    "        \n",
    "        axes[1,0].set_xlabel('Training Time (minutes)')\n",
    "        axes[1,0].set_ylabel('WER (%)')\n",
    "        axes[1,0].set_title('Performance vs Training Time')\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Average Performance Summary\n",
    "    if 'model_type' in df.columns:\n",
    "        avg_metrics = df.groupby('model_type')[['final_wer', 'final_loss']].mean()\n",
    "        avg_metrics.plot(kind='bar', ax=axes[1,1], rot=45)\n",
    "        axes[1,1].set_title('Average Performance Metrics')\n",
    "        axes[1,1].set_ylabel('Value')\n",
    "        axes[1,1].legend(['WER (%)', 'Training Loss'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate performance plots\n",
    "if not metrics_df.empty:\n",
    "    plot_performance_comparison(metrics_df)\n",
    "else:\n",
    "    print(\"No data available for plotting. Please run training experiments first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b733e1",
   "metadata": {},
   "source": [
    "## 5. Generate Resource Usage Plots\n",
    "Plot resource usage metrics including memory consumption and parameter efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe54e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_resource_usage(df):\n",
    "    \"\"\"Plot resource usage comparison.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"No data available for resource plotting.\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Resource Usage Comparison: PEFT vs Full Fine-tuning', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Memory Usage\n",
    "    if 'peak_memory_gb' in df.columns and 'model_type' in df.columns:\n",
    "        sns.barplot(data=df, x='model_type', y='peak_memory_gb', ax=axes[0,0])\n",
    "        axes[0,0].set_title('Peak GPU Memory Usage')\n",
    "        axes[0,0].set_ylabel('Memory (GB)')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, bar in enumerate(axes[0,0].patches):\n",
    "            height = bar.get_height()\n",
    "            axes[0,0].text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                          f'{height:.1f}GB', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Trainable Parameters\n",
    "    if 'trainable_params_millions' in df.columns and 'model_type' in df.columns:\n",
    "        sns.barplot(data=df, x='model_type', y='trainable_params_millions', ax=axes[0,1])\n",
    "        axes[0,1].set_title('Trainable Parameters')\n",
    "        axes[0,1].set_ylabel('Parameters (Millions)')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, bar in enumerate(axes[0,1].patches):\n",
    "            height = bar.get_height()\n",
    "            axes[0,1].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                          f'{height:.1f}M', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Training Time by Dialect\n",
    "    if 'training_time_minutes' in df.columns and 'dialect' in df.columns:\n",
    "        sns.barplot(data=df, x='dialect', y='training_time_minutes', hue='model_type', ax=axes[1,0])\n",
    "        axes[1,0].set_title('Training Time by Dialect')\n",
    "        axes[1,0].set_ylabel('Time (minutes)')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Efficiency Score (Performance per GB)\n",
    "    if 'efficiency_score' in df.columns and 'model_type' in df.columns:\n",
    "        sns.boxplot(data=df, x='model_type', y='efficiency_score', ax=axes[1,1])\n",
    "        axes[1,1].set_title('Efficiency Score\\n(Performance per GB Memory)')\n",
    "        axes[1,1].set_ylabel('Efficiency Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate resource usage plots\n",
    "if not metrics_df.empty:\n",
    "    plot_resource_usage(metrics_df)\n",
    "else:\n",
    "    print(\"No data available for plotting. Please run training experiments first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82749847",
   "metadata": {},
   "source": [
    "## 6. Create Efficiency Summary Table\n",
    "Generate a summary table comparing PEFT vs Full Fine-tuning efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d53d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_efficiency_summary(df):\n",
    "    \"\"\"Create efficiency summary table.\"\"\"\n",
    "    if df.empty or 'model_type' not in df.columns:\n",
    "        print(\"No data available for efficiency summary.\")\n",
    "        return\n",
    "    \n",
    "    # Group by model type and calculate averages\n",
    "    summary_cols = ['final_wer', 'training_time_minutes', 'peak_memory_gb', \n",
    "                   'trainable_params_millions', 'total_params_millions']\n",
    "    \n",
    "    available_cols = [col for col in summary_cols if col in df.columns]\n",
    "    \n",
    "    if not available_cols:\n",
    "        print(\"No relevant columns available for summary.\")\n",
    "        return\n",
    "    \n",
    "    summary = df.groupby('model_type')[available_cols].mean().round(2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EFFICIENCY SUMMARY: PEFT LoRA vs Full Fine-tuning\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Display the summary table\n",
    "    print(summary.to_string())\n",
    "    \n",
    "    # Calculate improvement percentages\n",
    "    if len(summary) >= 2:\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        print(\"PEFT LoRA IMPROVEMENTS OVER FULL FINE-TUNING:\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        peft_row = summary.loc[summary.index.str.contains('PEFT', case=False, na=False)]\n",
    "        full_row = summary.loc[summary.index.str.contains('Full', case=False, na=False)]\n",
    "        \n",
    "        if not peft_row.empty and not full_row.empty:\n",
    "            peft_values = peft_row.iloc[0]\n",
    "            full_values = full_row.iloc[0]\n",
    "            \n",
    "            # Calculate improvements (negative = better for WER, time, memory)\n",
    "            improvements = {\n",
    "                'WER': ((peft_values.get('final_wer', 0) - full_values.get('final_wer', 0)) / full_values.get('final_wer', 1)) * 100,\n",
    "                'Training Time': ((peft_values.get('training_time_minutes', 0) - full_values.get('training_time_minutes', 0)) / full_values.get('training_time_minutes', 1)) * 100,\n",
    "                'Memory Usage': ((peft_values.get('peak_memory_gb', 0) - full_values.get('peak_memory_gb', 0)) / full_values.get('peak_memory_gb', 1)) * 100,\n",
    "                'Trainable Parameters': ((peft_values.get('trainable_params_millions', 0) - full_values.get('trainable_params_millions', 0)) / full_values.get('trainable_params_millions', 1)) * 100\n",
    "            }\n",
    "            \n",
    "            for metric, improvement in improvements.items():\n",
    "                if not np.isnan(improvement) and abs(improvement) < 1000:  # Sanity check\n",
    "                    sign = \"↓\" if improvement < 0 else \"↑\"\n",
    "                    print(f\"{metric:20}: {improvement:+6.1f}% {sign}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Generate efficiency summary\n",
    "if not metrics_df.empty:\n",
    "    create_efficiency_summary(metrics_df)\n",
    "else:\n",
    "    print(\"No data available for summary. Please run training experiments first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f344e01",
   "metadata": {},
   "source": [
    "## 7. Generate Model Performance Comparison Charts\n",
    "Create detailed comparison charts showing performance across different dialects and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ecf5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detailed_comparison(df):\n",
    "    \"\"\"Create detailed comparison charts.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"No data available for detailed comparison.\")\n",
    "        return\n",
    "    \n",
    "    # Create a comprehensive comparison figure\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Performance by Dialect (Large subplot)\n",
    "    ax1 = plt.subplot(3, 3, (1, 4))\n",
    "    if 'dialect' in df.columns and 'final_wer' in df.columns:\n",
    "        dialect_performance = df.groupby(['dialect', 'model_type'])['final_wer'].mean().unstack()\n",
    "        dialect_performance.plot(kind='bar', ax=ax1, rot=45, width=0.8)\n",
    "        ax1.set_title('WER Performance by Dialect', fontsize=14, fontweight='bold')\n",
    "        ax1.set_ylabel('Word Error Rate (%)')\n",
    "        ax1.legend(title='Method')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Memory vs Performance Scatter\n",
    "    ax2 = plt.subplot(3, 3, 3)\n",
    "    if 'peak_memory_gb' in df.columns and 'final_wer' in df.columns:\n",
    "        for model_type, group in df.groupby('model_type'):\n",
    "            ax2.scatter(group['peak_memory_gb'], group['final_wer'], \n",
    "                       label=model_type, alpha=0.7, s=80)\n",
    "        ax2.set_xlabel('Memory (GB)')\n",
    "        ax2.set_ylabel('WER (%)')\n",
    "        ax2.set_title('Memory vs Performance')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Parameter Efficiency\n",
    "    ax3 = plt.subplot(3, 3, 6)\n",
    "    if 'trainable_percentage' in df.columns and 'model_type' in df.columns:\n",
    "        sns.barplot(data=df, x='model_type', y='trainable_percentage', ax=ax3)\n",
    "        ax3.set_title('Parameter Efficiency')\n",
    "        ax3.set_ylabel('Trainable Parameters (%)')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Training Time Distribution\n",
    "    ax4 = plt.subplot(3, 3, 7)\n",
    "    if 'training_time_minutes' in df.columns and 'model_type' in df.columns:\n",
    "        df.boxplot(column='training_time_minutes', by='model_type', ax=ax4)\n",
    "        ax4.set_title('Training Time Distribution')\n",
    "        ax4.set_ylabel('Time (minutes)')\n",
    "        ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 5. Overall Efficiency Radar Chart (if we have multiple metrics)\n",
    "    ax5 = plt.subplot(3, 3, 8)\n",
    "    if 'model_type' in df.columns and len(df['model_type'].unique()) >= 2:\n",
    "        # Create a simple efficiency comparison\n",
    "        metrics = ['final_wer', 'training_time_minutes', 'peak_memory_gb']\n",
    "        available_metrics = [m for m in metrics if m in df.columns]\n",
    "        \n",
    "        if available_metrics:\n",
    "            efficiency_data = df.groupby('model_type')[available_metrics].mean()\n",
    "            # Normalize metrics (lower is better for all these metrics)\n",
    "            normalized_data = 100 / efficiency_data  # Inverse so higher is better\n",
    "            \n",
    "            normalized_data.plot(kind='bar', ax=ax5, rot=45)\n",
    "            ax5.set_title('Efficiency Comparison\\n(Higher = Better)')\n",
    "            ax5.set_ylabel('Efficiency Score')\n",
    "            ax5.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # 6. Summary Statistics\n",
    "    ax6 = plt.subplot(3, 3, 9)\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    # Create summary text\n",
    "    summary_text = \"EXPERIMENT SUMMARY\\n\\n\"\n",
    "    if not df.empty:\n",
    "        summary_text += f\"Total Experiments: {len(df)}\\n\"\n",
    "        if 'dialect' in df.columns:\n",
    "            summary_text += f\"Dialects: {', '.join(df['dialect'].unique())}\\n\"\n",
    "        if 'model_type' in df.columns:\n",
    "            summary_text += f\"Methods: {', '.join(df['model_type'].unique())}\\n\\n\"\n",
    "        \n",
    "        # Best performance\n",
    "        if 'final_wer' in df.columns:\n",
    "            best_wer = df.loc[df['final_wer'].idxmin()]\n",
    "            summary_text += f\"Best WER: {best_wer['final_wer']:.2f}%\\n\"\n",
    "            summary_text += f\"Method: {best_wer.get('model_type', 'N/A')}\\n\"\n",
    "            summary_text += f\"Dialect: {best_wer.get('dialect', 'N/A')}\\n\"\n",
    "    \n",
    "    ax6.text(0.1, 0.9, summary_text, transform=ax6.transAxes, \n",
    "             fontsize=11, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    plt.suptitle('Comprehensive PEFT vs Full Fine-tuning Analysis', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate detailed comparison\n",
    "if not metrics_df.empty:\n",
    "    plot_detailed_comparison(metrics_df)\n",
    "else:\n",
    "    print(\"No data available for plotting. Please run training experiments first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2bc542",
   "metadata": {},
   "source": [
    "## 8. Save Analysis Results\n",
    "Save the analysis results and plots for documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466525a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_analysis_results(df, output_dir=\"./analysis_results\"):\n",
    "    \"\"\"Save analysis results to files.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save processed data\n",
    "    csv_path = output_path / \"training_metrics_processed.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Processed data saved to: {csv_path}\")\n",
    "    \n",
    "    # Save summary statistics\n",
    "    if 'model_type' in df.columns:\n",
    "        summary_cols = ['final_wer', 'training_time_minutes', 'peak_memory_gb', \n",
    "                       'trainable_params_millions']\n",
    "        available_cols = [col for col in summary_cols if col in df.columns]\n",
    "        \n",
    "        if available_cols:\n",
    "            summary = df.groupby('model_type')[available_cols].agg(['mean', 'std']).round(3)\n",
    "            summary_path = output_path / \"efficiency_summary.csv\"\n",
    "            summary.to_csv(summary_path)\n",
    "            print(f\"Summary statistics saved to: {summary_path}\")\n",
    "    \n",
    "    print(f\"\\nAll analysis results saved to: {output_path}\")\n",
    "\n",
    "# Save results\n",
    "if not metrics_df.empty:\n",
    "    save_analysis_results(metrics_df)\n",
    "    \n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"\\nTo generate this analysis:\")\n",
    "print(\"1. Run training experiments with the updated script\")\n",
    "print(\"2. The script automatically saves metrics_*.json files\")\n",
    "print(\"3. This notebook loads and analyzes all available metrics\")\n",
    "print(\"4. Comparison plots show PEFT vs Full Fine-tuning efficiency\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
