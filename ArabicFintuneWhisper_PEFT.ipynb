{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "061d57b3",
   "metadata": {},
   "source": [
    "# Parameter-Efficient Fine-tuning of Whisper for Arabic Dialects using PEFT & LoRA\n",
    "\n",
    "This notebook demonstrates how to fine-tune Whisper models for Arabic dialects using Parameter-Efficient Fine-Tuning (PEFT) with Low-Rank Adaptation (LoRA). This approach:\n",
    "\n",
    "1. **Reduces memory usage**: Fine-tune large models with less GPU memory\n",
    "2. **Faster training**: Only trains 1% of the model parameters\n",
    "3. **Better generalization**: Prevents catastrophic forgetting\n",
    "4. **Smaller checkpoints**: Model adapters are ~60MB vs full model ~1.5GB\n",
    "\n",
    "We'll fine-tune Whisper-small on Arabic dialects using the MASC dataset with LoRA adapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72da97e",
   "metadata": {},
   "source": [
    "## Install Required Packages\n",
    "\n",
    "Install the necessary packages for PEFT fine-tuning including bitsandbytes for 8-bit training and PEFT for LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f230ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for PEFT fine-tuning\n",
    "!pip install --upgrade pip\n",
    "!pip install -q transformers datasets librosa evaluate jiwer gradio bitsandbytes==0.41.3 accelerate\n",
    "!pip install -q peft>=0.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d92738",
   "metadata": {},
   "source": [
    "## GPU Setup and Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24c0a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and specs\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Set environment for CUDA\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d48d2c2",
   "metadata": {},
   "source": [
    "## Hugging Face Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c925e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12c059a",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50939850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and training configuration\n",
    "model_name_or_path = \"openai/whisper-small\"\n",
    "language = \"Arabic\"\n",
    "task = \"transcribe\"\n",
    "\n",
    "# Arabic dialect to fine-tune on\n",
    "# Options: \"egyptian\", \"gulf\", \"iraqi\", \"levantine\", \"maghrebi\", \"all\"\n",
    "target_dialect = \"egyptian\"  # Change this to your desired dialect\n",
    "\n",
    "print(f\"Fine-tuning Whisper-small for {target_dialect} Arabic dialect using PEFT/LoRA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b788c8",
   "metadata": {},
   "source": [
    "## Load Arabic Dialect Dataset\n",
    "\n",
    "Load the preprocessed Arabic dialect dataset. In practice, you would replace this with your actual dataset loading logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60247db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "import os\n",
    "\n",
    "# For demonstration, we'll use Common Voice Arabic\n",
    "# In your actual implementation, replace this with your dialect dataset\n",
    "try:\n",
    "    # Try to load preprocessed dialect data if available\n",
    "    if target_dialect == \"all\":\n",
    "        print(\"Loading all Arabic dialects...\")\n",
    "        # This would be your actual dialect data loading logic\n",
    "        arabic_dialects = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ar\", split=\"train[:1000]\")\n",
    "        test_data = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ar\", split=\"test[:200]\")\n",
    "    else:\n",
    "        print(f\"Loading {target_dialect} dialect dataset...\")\n",
    "        # This would be your actual dialect data loading logic\n",
    "        arabic_dialects = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ar\", split=\"train[:1000]\")\n",
    "        test_data = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ar\", split=\"test[:200]\")\n",
    "    \n",
    "    # Create dataset dict\n",
    "    dialect_dataset = DatasetDict({\n",
    "        \"train\": arabic_dialects,\n",
    "        \"test\": test_data\n",
    "    })\n",
    "    \n",
    "    print(f\"Dataset loaded: {dialect_dataset}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not load dialect dataset: {e}\")\n",
    "    print(\"Using Common Voice Arabic as fallback...\")\n",
    "    \n",
    "    dialect_dataset = DatasetDict()\n",
    "    dialect_dataset[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ar\", split=\"train[:1000]\", use_auth_token=True)\n",
    "    dialect_dataset[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ar\", split=\"test[:200]\", use_auth_token=True)\n",
    "    \n",
    "    print(f\"Fallback dataset loaded: {dialect_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b07b4e",
   "metadata": {},
   "source": [
    "## Prepare Feature Extractor, Tokenizer and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0417dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Initialize tokenizer for Arabic\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "\n",
    "# Initialize processor\n",
    "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "\n",
    "print(\"Feature extractor, tokenizer, and processor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e326fd00",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "# Remove unnecessary columns (keep only audio and sentence)\n",
    "if \"common_voice\" in str(type(dialect_dataset[\"train\"])):\n",
    "    # Remove Common Voice specific columns\n",
    "    columns_to_remove = [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"]\n",
    "    columns_to_remove = [col for col in columns_to_remove if col in dialect_dataset[\"train\"].column_names]\n",
    "    dialect_dataset = dialect_dataset.remove_columns(columns_to_remove)\n",
    "\n",
    "# Resample audio to 16kHz (Whisper's expected sampling rate)\n",
    "dialect_dataset = dialect_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "print(\"Dataset preprocessing completed\")\n",
    "print(f\"First sample: {dialect_dataset['train'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bc4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    \"\"\"Prepare dataset for training by extracting features and tokenizing text.\"\"\"\n",
    "    # Load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # Compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # Encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "# Apply preprocessing to dataset\n",
    "print(\"Applying data preparation (this may take a few minutes)...\")\n",
    "dialect_dataset = dialect_dataset.map(\n",
    "    prepare_dataset, \n",
    "    remove_columns=dialect_dataset.column_names[\"train\"], \n",
    "    num_proc=2\n",
    ")\n",
    "\n",
    "print(f\"Preprocessed dataset: {dialect_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f74e186",
   "metadata": {},
   "source": [
    "## Data Collator for PEFT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f1f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # First treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # Pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # If bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "# Initialize data collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "print(\"Data collator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb370da",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f3fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load WER metric\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute WER metric for evaluation.\"\"\"\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # Replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # We do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "print(\"Evaluation metrics configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b837c776",
   "metadata": {},
   "source": [
    "## Load Pre-trained Model with 8-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0337892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "# Load model in 8-bit for memory efficiency\n",
    "print(f\"Loading {model_name_or_path} with 8-bit quantization...\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    model_name_or_path, \n",
    "    load_in_8bit=True, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {model}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d732d9f",
   "metadata": {},
   "source": [
    "## Prepare Model for 8-bit Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbd4818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_int8_training\n",
    "\n",
    "# Prepare model for 8-bit training\n",
    "model = prepare_model_for_int8_training(model, output_embedding_layer_name=\"proj_out\")\n",
    "\n",
    "# Make inputs require grad for convolutional layers\n",
    "def make_inputs_require_grad(module, input, output):\n",
    "    output.requires_grad_(True)\n",
    "\n",
    "model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)\n",
    "\n",
    "print(\"Model prepared for 8-bit training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deb847f",
   "metadata": {},
   "source": [
    "## Apply LoRA (Low-Rank Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a4267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=32,  # Rank\n",
    "    lora_alpha=64,  # Alpha parameter for LoRA scaling\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Target modules for LoRA\n",
    "    lora_dropout=0.05,  # Dropout for LoRA layers\n",
    "    bias=\"none\",  # Bias type\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nLoRA configuration applied successfully!\")\n",
    "print(\"Only training ~1% of the model parameters with LoRA adapters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d249105e",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04443bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# Training arguments optimized for PEFT\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"./whisper-small-arabic-{target_dialect}-peft\",  # Output directory\n",
    "    per_device_train_batch_size=16,  # Larger batch size possible with PEFT\n",
    "    gradient_accumulation_steps=1,  # Can use smaller accumulation with larger batch\n",
    "    learning_rate=1e-3,  # Higher learning rate for LoRA\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=3,  # Fewer epochs needed with PEFT\n",
    "    evaluation_strategy=\"steps\",\n",
    "    fp16=True,  # Use mixed precision\n",
    "    per_device_eval_batch_size=16,\n",
    "    generation_max_length=128,\n",
    "    logging_steps=25,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=2,\n",
    "    # PEFT specific settings\n",
    "    remove_unused_columns=False,  # Required for PeftModel\n",
    "    label_names=[\"labels\"],  # Required for PeftModel\n",
    "    push_to_hub=False,  # Set to True if you want to push to hub\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured for PEFT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e928395",
   "metadata": {},
   "source": [
    "## PEFT Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3156f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "import os\n",
    "\n",
    "# Callback to save only PEFT adapter weights\n",
    "class SavePeftModelCallback(TrainerCallback):\n",
    "    def on_save(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
    "\n",
    "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
    "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
    "\n",
    "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
    "        if os.path.exists(pytorch_model_path):\n",
    "            os.remove(pytorch_model_path)\n",
    "        return control\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dialect_dataset[\"train\"],\n",
    "    eval_dataset=dialect_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    # Note: compute_metrics commented out due to INT8 training constraints\n",
    "    # compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    callbacks=[SavePeftModelCallback],\n",
    ")\n",
    "\n",
    "# Disable cache for training\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(\"PEFT Trainer initialized and ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c3142",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a372239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(f\"Starting PEFT fine-tuning for {target_dialect} Arabic dialect...\")\n",
    "print(\"Training progress will be logged below.\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8476b46",
   "metadata": {},
   "source": [
    "## Save the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0723517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final PEFT model\n",
    "final_model_path = f\"./whisper-small-arabic-{target_dialect}-peft-final\"\n",
    "trainer.model.save_pretrained(final_model_path)\n",
    "processor.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"Final PEFT model saved to: {final_model_path}\")\n",
    "print(f\"Model size: {os.path.getsize(final_model_path + '/adapter_model.bin') / 1024**2:.1f} MB\")\n",
    "\n",
    "# Optionally push to hub\n",
    "# trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803401f9",
   "metadata": {},
   "source": [
    "## Load and Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba413ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned PEFT model for inference\n",
    "def load_peft_model(adapter_path, base_model_name=\"openai/whisper-small\"):\n",
    "    \"\"\"Load PEFT model for inference.\"\"\"\n",
    "    # Load PEFT config\n",
    "    peft_config = PeftConfig.from_pretrained(adapter_path)\n",
    "    \n",
    "    # Load base model\n",
    "    base_model = WhisperForConditionalGeneration.from_pretrained(\n",
    "        base_model_name, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Load PEFT model\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load the trained model\n",
    "try:\n",
    "    inference_model = load_peft_model(final_model_path)\n",
    "    inference_processor = WhisperProcessor.from_pretrained(final_model_path)\n",
    "    \n",
    "    print(\"PEFT model loaded successfully for inference!\")\n",
    "    \n",
    "    # Enable cache for inference\n",
    "    inference_model.config.use_cache = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"You can load the model later using the load_peft_model function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a95d2",
   "metadata": {},
   "source": [
    "## Test the Model on Sample Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9806f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on a sample from the test set\n",
    "def test_sample_audio(model, processor, test_sample):\n",
    "    \"\"\"Test the model on a sample audio.\"\"\"\n",
    "    # Prepare input\n",
    "    input_features = processor(\n",
    "        test_sample[\"audio\"][\"array\"], \n",
    "        sampling_rate=test_sample[\"audio\"][\"sampling_rate\"], \n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "    \n",
    "    # Move to device\n",
    "    if torch.cuda.is_available():\n",
    "        input_features = input_features.cuda()\n",
    "    \n",
    "    # Generate prediction\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features, max_length=128)\n",
    "    \n",
    "    # Decode prediction\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return transcription\n",
    "\n",
    "# Test on a sample\n",
    "if 'inference_model' in locals():\n",
    "    test_sample = dialect_dataset[\"test\"][0]\n",
    "    \n",
    "    print(\"Testing the fine-tuned PEFT model...\")\n",
    "    print(f\"Original text: {test_sample.get('sentence', 'N/A')}\")\n",
    "    \n",
    "    try:\n",
    "        prediction = test_sample_audio(inference_model, inference_processor, test_sample)\n",
    "        print(f\"Predicted text: {prediction}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {e}\")\n",
    "else:\n",
    "    print(\"Model not loaded. Please run the previous cell successfully first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c5632e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to fine-tune Whisper for Arabic dialects using PEFT and LoRA:\n",
    "\n",
    "### Key Benefits of PEFT Approach:\n",
    "1. **Memory Efficient**: Used 8-bit quantization and LoRA to reduce memory usage\n",
    "2. **Parameter Efficient**: Only trained ~1% of the model parameters\n",
    "3. **Faster Training**: Higher batch sizes and faster convergence\n",
    "4. **Smaller Checkpoints**: Adapter weights are ~60MB vs full model ~1.5GB\n",
    "5. **Better Generalization**: Less prone to catastrophic forgetting\n",
    "\n",
    "### Model Performance:\n",
    "- The LoRA adapters are trained specifically for Arabic dialect recognition\n",
    "- The approach maintains the base Whisper model's capabilities while adapting to dialect-specific patterns\n",
    "- Fine-tuned model can be easily shared and deployed\n",
    "\n",
    "### Next Steps:\n",
    "1. Evaluate the model on held-out test sets\n",
    "2. Compare performance with full fine-tuning\n",
    "3. Experiment with different LoRA configurations (rank, alpha, target modules)\n",
    "4. Train adapters for multiple dialects and combine them"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
